{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \n",
    "### <center> Author: Archit Rungta\n",
    "    \n",
    "## <center> Tutorial\n",
    "## <center> Imputing missing data with fancyimpute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi folks!\n",
    "\n",
    "Often in real world applications of data analysis, we run into the problem of missing data. This can happen due to a multitude of reasons such as:\n",
    " - The data was compiled from different sources/times \n",
    " - Corrupted during storage\n",
    " - Certain fields were optional\n",
    " - etc.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has the following sections:\n",
    " 1. Introduction\n",
    " 2. The Problem\n",
    " 3. KNN Imputation\n",
    " 4. Comparison And Application\n",
    " 5. Summary\n",
    " 6. Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we look at the problem of missing data in data analytics. Then, we categorize the different types of missing data and briefly discuss the specific issue presented by each specific type. Finally, we look at various methods of handling data imputation and compare their accuracy on a real-world dataset with logistic regression. We also look at the validity of a commonly held assumption about imputation techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly, missing data is classified into 3 categories. \n",
    " - Missing Completely At Random (MCAR)\n",
    " > Values in a data set are missing completely at random (MCAR) if the events that lead to any particular data-item being missing are independent both of observable variables and of unobservable parameters of interest, and occur entirely at random\n",
    " - Missing At Random (MAR)\n",
    " >Missing at random (MAR) occurs when the missingness is not random, but where missingness can be fully accounted for by variables where there is complete information\n",
    " - Missing Not At Random (MNAR)\n",
    " >Missing not at random (MNAR) (also known as nonignorable nonresponse) is data that is neither MAR nor MCAR\n",
    " \n",
    "Data compilation from different sources is an example of MAR while data corruption is an example of MCAR. MNAR is not a problem we can fix with imputation because this is **non-ignorable non-response.** The only thing we can do about MNAR is to gather more information from different sources or ignore it all-together. As such we are not going to talk about MNAR anymore in this tutorial. \n",
    "\n",
    "All of the techniques that follow are applicable only for MCAR. However, in real world scenarios, MAR is more common. As such, we will treat MAR as MCAR only which gives a reasonably good approximation in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <center> The Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a toy example, \n",
    "\n",
    "\\begin{align}\n",
    "\\ y & = \\sin(x) x\\, \\text{for $|x|<=6$}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1b64f722898>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-6,6)\n",
    "y = np.asarray([x1*np.sin(x1) for x1 in x])\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete some points on random to get an MCAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fraction = 0.3\n",
    "indices = np.random.randint(1,len(x)-1, size=int((1-missing_fraction)*len(x)))\n",
    "x_mcar = x[indices]\n",
    "y_mcar = y[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1b64f5e34a8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_mcar,y_mcar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial, we will use MSE as an indicator of how good an imputation technique is when we have the original dataset and accuracy on predictions when we don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the easiest methods first:\n",
    " - Mean\n",
    " - Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3613103837114537"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEwlJREFUeJzt3X+MHGd9x/HPpxdTtvzopYpR8NnGUMAUcIrpElpF/UESuJRCMEaVSFWKSqVraUFBAkMcq2BUoUS45YcAtbIg7R9NQRUxDoJQkxRUVFRSznGIE4wRQkC8DuIQOn4oV2I73/6x58QO++Nm5tmdnWffL8mSd3Zunu/M7nxu7pl5ZhwRAgDk45fqLgAAkBbBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGYIdADJDsANAZgh2AMjMBXU0etFFF8WWLVvqaBoAGuvw4cM/jIj1w+arJdi3bNmixcXFOpoGgMay/d21zFe5K8b2JttftH3M9n22r626TABAeSmO2E9LemtE3GX7SZIO2749Ir6eYNkAgIIqH7FHxAMRcdfq/38q6ZikuarLBQCUk/SqGNtbJG2XdGeP9xZsL9peXFpaStksAOAcyYLd9hMl3SLpLRHxk8e+HxH7I6IdEe3164ee1AUAlJQk2G2vUzfUb46IAymWCQAop/LJU9uW9DFJxyLifdVLQhMcPNLRvkPHdXJ5RRtmW9o1v1U7tnNqBZgEKa6KuUzS6yQdtX336rTrI+K2BMvGBDp4pKPdB45q5dQZSVJneUW7DxyVJMK9BvySxWNVDvaI+G9JTlALGmLfoeOPhPpZK6fOaN+h4wTKmPFLFr3UMvIUzXZyeaXQdIxOmV+yHOHnj5uAobANs61C0zE6RX/Jnj3C7yyvKPToEf7BI50RVolxI9hR2K75rWqtmzlvWmvdjHbNb62poulV9JfsoCN85IOuGBR29s/2In/O8+d/db224a75ref1sUuDf8nSjTYdHBFjb7Tdbgd3d5wejz3BJ3XD54ad2wj3NRq0DaW1/5K97MYvqNMjxOdmW/rydZePpngkY/twRLSHzccRO0aOq2iqG7QNv3zd5WvejkWP8NFMBDtGjj//q0u1Dct0o6F5CHaM3IbZVs8//7mKZu1SbsMd2+cI8sxxVQxGjqtoqmMbogiO2DFy/PlfHdsQRXBVDAA0xFqviqErBgAyQ1cMasXAJSA9gh214c6EwGjQFYPacN8SYDQIdtSGgUvAaBDsqA23/wVGg2BHbRh0A4wGJ09RGwbdAKNBsKNW3LcESC9JV4ztm2z/wPa9KZYHACgvVR/7v0i6KtGyAAAVJAn2iPiSpB+lWBYAoBquigGAzIwt2G0v2F60vbi0tDSuZgFg6owt2CNif0S0I6K9fv36cTULAFOHrhgAyEyqyx0/Lul/JG21fcL2X6RYLgCguCQDlCLimhTLAQBUR1cMAGSGYAeAzHCvGGCEePQf6kCwAyPCo/9QF7pigBHh0X+oC8EOjAiP/kNdCHZgRHj0H+pCsAMjwqP/UBdOngIjwqP/UBeCHRghHv2HOtAVAwCZ4Ygd2ahzMBADkTBJCHZkoc7BQAxEwqShKwZZqHMwEAORMGk4Ys/QNHYL1DkYiIFImDQcsWfmbLdAZ3lFoUe7BQ4e6dRd2kjVORiIgUiYNAR7Zqa1W6DOwUAMRMKkoSsmM9PaLVDnYCAGImHSEOyZ2TDbUqdHiE9Dt0C/wUApzzn0WxYDkTBJ6IrJDN0C50t5zmFaz1+geQj2zOzYPqcbdm7T3GxLljQ329INO7dN7dFkynMO03r+As2TpCvG9lWSPihpRtJHI+LGFMtFOXQLPGrQOYd+3Sr9pk/r+Qs0T+Vgtz0j6SOSXirphKSv2v50RHy96rKBqvqdc/jV1rqeo0UXv/sj3XK403MU6TSfv0CzpOiKuVTStyLi2xHxkKRPSHpVguUClfU752CrZ7fKx++8v293C+cv0BQpgn1O0v3nvD6xOg2oXb9zDssPnuo5/5mIntNPLq9w/gKNkaKP3T2m/cLeYXtB0oIkbd68OUGzwNr0Ouew79Dxnt0qM3bPcD/b3cL5CzRBiiP2E5I2nfN6o6STj50pIvZHRDsi2uvXr0/QLFBev26Va168ie4WNF6KI/avSnqW7adL6kh6raQ/SbBcYGQGjRZtP+3XGEWKRnP06VMstBD75ZI+oO7ljjdFxHsGzd9ut2NxcbFyuwAwTWwfjoj2sPmSXMceEbdJui3FsgAA1TDyFAAyQ7ADQGYIdgDIDMEOAJkh2AEgMwQ7AGSGYAeAzBDsAJAZgh0AMkOwA0BmCHYAyAzBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGYIdADJDsANAZgh2AMgMwQ4AmSHYASAzlYLd9h/bvs/2w7bbqYoCAJRX9Yj9Xkk7JX0pQS0AgAQuqPLDEXFMkmynqQYAUBl97ACQmaFH7LbvkHRxj7f2RMSta23I9oKkBUnavHnzmgsEABQzNNgj4soUDUXEfkn7JandbkeKZQIAfhFdMQCQmUonT22/WtKHJK2X9Fnbd0fEfJLKHuPgkY72HTquk8sr2jDb0q75rdqxfW7ge4N+pmgbo1ZmHVKtX6rtV6amMsup83NKJeU2L9pGmflTfXdSfs+btL+Ou1ZHjL9XpN1ux+Li4prnP3iko90Hjmrl1JlHprXWzeiGndskqed7r/mtOd1yuNPzZ3pt0EFtjPrL0q/tQesg9V7vouvXr42i229QG0W3YZnPexyfUyplPu+i61b0s0i5j6WannL/Tmkc+1I/tg9HxNAxQ40I9stu/II6yyu/MH1utiVJPd+bsXWmx7rNzbb05esuL9RGr/lT6tf2oHWQeq930fXr10bR7TeojaLbsMznPY7PKZUyn3fRdSv6WaTcx1JNT7l/pzSOfamftQZ7pa6YcTnZYyMOmi6p54Yss6xBbaTSr42i6zDovaJtjKPtlMsZx+eUSsrPu2gbKb//Rb87Kb9rKbdVUePYl6pqxMnTDau/uXtN7/feTJ9BU4OWVWR6SmXWIdV69Guj6PYr03aZ5dT5OaWS6jtbpo0y01N9d8p818axrYoax75UVSOCfdf8VrXWzZw3rbVuRrvmt/Z975oXb+r7M0XbGLUy61C03qJtFN1+g9ooug3LfN7j+JxSSfWdLdNGme9/qu9Ome/aOLZVUePYl6qa2bt378gW3s/+/fv3LiwsrHn+5zz1ydp4YUtHOz/Wz/7vtOZmW3rnK5+rHdvn+r731y95Zt+fKdrGqJVZh6L1Fm2j6PYb1EbRbVjm827KiVOp3Oedqo0y3/9U350y37VxbKtU2zblvtTPu9/97gf27t27f9h8jTh5CgBY+8nTRnTFAADWjmAHgMwQ7ACQGYIdADJDsANAZgh2AMgMwQ4AmSHYASAzBDsAZIZgB4DMEOwAkBmCHQAyQ7ADQGYIdgDIDMEOAJmpFOy299n+hu17bH/K9myqwgAA5VQ9Yr9d0vMj4hJJ35S0u3pJAIAqKgV7RHw+Ik6vvvyKpI3VSwIAVJGyj/0Nkj7X703bC7YXbS8uLS0lbBYAcK4Lhs1g+w5JF/d4a09E3Lo6zx5JpyXd3G85EbFf0n6p+8zTUtUCAIYaGuwRceWg922/XtIrJF0RdTwZGwBwnqHBPojtqyS9Q9LvR8SDaUoCAFRRtY/9w5KeJOl223fb/qcENQEAKqh0xB4Rz0xVCAAgDUaeAkBmCHYAyAzBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGYIdADJDsANAZgh2AMgMwQ4AmSHYASAzBDsAZIZgB4DMEOwAkBmCHQAyQ7ADQGYIdgDIDMEOAJkh2AEgMwQ7AGSmUrDb/jvb99i+2/bnbW9IVRgAoJyqR+z7IuKSiHiBpM9IemeCmgAAFVQK9oj4yTkvnyApqpUDAKjqgqoLsP0eSX8m6ceSXjJgvgVJC5K0efPmqs0CAPpwxOCDbNt3SLq4x1t7IuLWc+bbLenxEfGuYY222+1YXFwsWisATDXbhyOiPWy+oUfsEXHlGtv8N0mflTQ02KfZwSMd7Tt0XCeXV7RhtqVd81u1Y/tc3WUByEjVq2Kedc7LqyV9o1o5eTt4pKPdB46qs7yikNRZXtHuA0d18Ein7tIAZKTqVTE32r7X9j2SXibp2gQ1ZWvfoeNaOXXmvGkrp85o36HjNVUEIEeVTp5GxGtSFTINTi6vFJoOAGUw8nSMNsy2Ck0HgDII9jHaNb9VrXUz501rrZvRrvmtNVUEIEeVr2PH2p29+oWrYgCMEsE+Zju2zxHkAEaKrhgAyAzBDgCZIdgBIDMEOwBkhmAHgMwQ7ACQGYIdADJDsANAZgh2AMgMwQ4AmSHYASAzBDsAZIZgB4DMEOwAkBlu2zsiB490uO86gFoQ7CNw8EhHuw8cfeTB1Z3lFe0+cFSSCHcAI5ekK8b222yH7YtSLK/p9h06/kion7Vy6oz2HTpeU0UApknlYLe9SdJLJX2vejl5OLm8Umg6AKSU4oj9/ZLeLikSLCsLG2ZbhaYDQEqVgt321ZI6EfG1RPVkYdf8VrXWzZw3rbVuRrvmt9ZUEYBpMvTkqe07JF3c4609kq6X9LK1NGR7QdKCJG3evLlAic1z9gQpV8UAqIMjyvWg2N4m6T8lPbg6aaOkk5IujYjvD/rZdrsdi4uLpdoFgGll+3BEtIfNV/pyx4g4Kukp5zT4HUntiPhh2WUCAKpj5CkAZCbZAKWI2JJqWQCA8jhiB4DMEOwAkBmCHQAyQ7ADQGYIdgDIDMEOAJmZyvuxp3wIBg/UACYL+/cUBnvKh2DwQA1gsrB/d01dV0zKh2DwQA1gsrB/d01dsKd8CAYP1AAmC/t319QFe8qHYPBADWCysH93TV2wp3wIBg/UACYL+3fX1J08TfkQDB6oAUwW9u+u0g/aqIIHbQBAcWt90MbUdcUAQO6mriumjKYOUgDwqGnajwn2IZo8SAFA16D9WGpmP/ogBPsQgwYpNP3DB6ZFv/1476fv089PP5zdgRt97EM0eZACgK5+++vyyqnGji4dhGAfosmDFAB0Fd1fm37gRrAP0eRBCgC6+u3HF/7Kup7zN/3ArVKw295ru2P77tV/L09V2KTYsX1ON+zcprnZlixpbralG3Zua3T/GzBt+u3H73rl87I8cKs0QMn2Xkk/i4i/L/JzDFACMCmadBnkWgcocVUMgKm2Y/vcxAZ5WSn62N9k+x7bN9m+MMHyAAAVDA1223fYvrfHv1dJ+kdJvy7pBZIekPQPA5azYHvR9uLS0lKyFQAAnC/ZTcBsb5H0mYh4/rB56WMHgOLGchMw20895+WrJd1bZXkAgOqqnjx9r+0XSApJ35H0l5UrAgBUUsv92G0vSfpuyR+/SNIPE5ZTJ9Zl8uSyHhLrMomqrsfTImL9sJlqCfYqbC+upY+pCViXyZPLekisyyQa13pwSwEAyAzBDgCZaWKw76+7gIRYl8mTy3pIrMskGst6NK6PHQAwWBOP2AEAAzQ22G2/2fZx2/fZfm/d9VRl+222w/ZFdddShu19tr+xet+gT9merbumomxftfqd+pbt6+qupyzbm2x/0fax1f3j2rprqsL2jO0jtj9Tdy1V2J61/cnV/eSY7d8ZVVuNDHbbL5H0KkmXRMTzJBW6bfCksb1J0kslfa/uWiq4XdLzI+ISSd+UtLvmegqxPSPpI5L+UNJzJV1j+7n1VlXaaUlvjYjfkPTbkv6mwesiSddKOlZ3EQl8UNJ/RMRzJP2mRrhOjQx2SW+UdGNE/FySIuIHNddT1fslvV3dEbyNFBGfj4jTqy+/ImljnfWUcKmkb0XEtyPiIUmfUPfgoXEi4oGIuGv1/z9VN0AaeV9a2xsl/ZGkj9ZdSxW2nyzp9yR9TJIi4qGIWB5Ve00N9mdL+l3bd9r+L9svqrugsmxfLakTEV+ru5aE3iDpc3UXUdCcpPvPeX1CDQ3Dc63enG+7pDvrraS0D6h70PNw3YVU9AxJS5L+ebVb6aO2nzCqxib2QRu275B0cY+39qhb94Xq/pn5Ikn/bvsZMaGX+AxZl+slvWy8FZUzaD0i4tbVefao2xVw8zhrS8A9pk3k92mtbD9R0i2S3hIRP6m7nqJsv0LSDyLisO0/qLueii6Q9EJJb46IO21/UNJ1kv52VI1NpIi4st97tt8o6cBqkP+v7YfVvQfDRN7ovd+62N4m6emSvmZb6nZf3GX70oj4/hhLXJNBn4kk2X69pFdIumJSf8kOcELSpnNeb5R0sqZaKrO9Tt1QvzkiDtRdT0mXSbp69VnKj5f0ZNv/GhF/WnNdZZyQdCIizv7l9El1g30kmtoVc1DS5ZJk+9mSHqcG3iAoIo5GxFMiYktEbFH3w3/hJIb6MLavkvQOSVdHxIN111PCVyU9y/bTbT9O0mslfbrmmkpx9yjhY5KORcT76q6nrIjYHREbV/eN10r6QkNDXav79P22zz4l+wpJXx9VexN7xD7ETZJusn2vpIckvb6BR4i5+bCkX5Z0++pfH1+JiL+qt6S1i4jTtt8k6ZCkGUk3RcR9NZdV1mWSXifpqO27V6ddHxG31VgTpDdLunn1wOHbkv58VA0x8hQAMtPUrhgAQB8EOwBkhmAHgMwQ7ACQGYIdADJDsANAZgh2AMgMwQ4Amfl/yp0BtA7DQ2QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_mean = np.array(y)\n",
    "for ind in list(set(np.linspace(0,len(x)-1))-set(indices)):\n",
    "    y_pred_mean[int(ind)] = np.mean(y_mcar)\n",
    "plt.scatter(x,y_pred_mean)\n",
    "mse(y_pred_mean,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.818580781176306"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_median = np.array(y)\n",
    "for ind in list(set(np.linspace(0,len(x)-1))-set(indices)):\n",
    "    y_pred_median[int(ind)] = np.median(y_mcar)\n",
    "plt.scatter(x,y_pred_median)\n",
    "mse(y_pred_median,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this seems like pretty awful. Let's see what fancyimpute has to offer. \n",
    "**Note: You need TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fancyimpute in c:\\programdata\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (1.14.3)\n",
      "Requirement already satisfied: keras>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (2.2.2)\n",
      "Requirement already satisfied: cvxpy>=1.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (1.0.9)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (1.11.0)\n",
      "Requirement already satisfied: np-utils in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (0.5.5.2)\n",
      "Requirement already satisfied: knnimpute in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (0.1.0)\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (0.19.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from fancyimpute) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.0.0->fancyimpute) (3.12)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.0.0->fancyimpute) (2.7.1)\n",
      "Requirement already satisfied: keras_applications==1.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.0.0->fancyimpute) (1.0.4)\n",
      "Requirement already satisfied: keras_preprocessing==1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=2.0.0->fancyimpute) (1.0.2)\n",
      "Requirement already satisfied: ecos>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (2.0.5)\n",
      "Requirement already satisfied: scs>=1.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (2.0.2)\n",
      "Requirement already satisfied: osqp in c:\\programdata\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (0.4.1)\n",
      "Requirement already satisfied: toolz in c:\\programdata\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (0.9.0)\n",
      "Requirement already satisfied: fastcache in c:\\programdata\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (1.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\lib\\site-packages (from cvxpy>=1.0.6->fancyimpute) (0.70.6.1)\n",
      "Requirement already satisfied: future>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from np-utils->fancyimpute) (0.16.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (0.2.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (1.12.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (0.31.1)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (39.1.0)\n",
      "Requirement already satisfied: tensorboard<1.11.0,>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (1.10.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (0.5.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow->fancyimpute) (3.6.0)\n",
      "Requirement already satisfied: dill>=0.2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from multiprocess->cvxpy>=1.0.6->fancyimpute) (0.2.8.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow->fancyimpute) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow->fancyimpute) (2.6.11)\n",
      "Requirement already satisfied: pyreadline>=1.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from dill>=0.2.8.1->multiprocess->cvxpy>=1.0.6->fancyimpute) (2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/50 with 0 missing, elapsed time: 0.003\n"
     ]
    }
   ],
   "source": [
    "y_pred_knn = np.concatenate((np.array(y).reshape(-1,1), np.array(x).reshape(-1,1)), axis=1)\n",
    "for ind in indices:\n",
    "    y_pred_knn[int(ind)] = [float(\"NaN\"), y_pred_knn[int(ind)][1]]\n",
    "y_pred_knn = fancyimpute.KNN(k=3).fit_transform(y_pred_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn_2 = [x[0] for x in y_pred_knn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2209843123701067"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y_pred_knn_2)\n",
    "mse(y_pred_knn_2,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, fancyimpute has performed much better than mean or median methods on this toy dataset. \n",
    "\n",
    "Next up, we get some in-depth understanding of how the KNN algorithm for fancyimpute works and apply it to some real datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <center> KNN Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In pattern recognition, the k-nearest neighbors algorithm is a non-parametric method used for classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption behind using KNN for missing values is that a point value can be approximated by the values of the points that are closest to it, based on other variables.\n",
    "\n",
    "The fancyimpute KNN algorithm works by calculating the k nearest neighbors which have the missing features available and then weights them based on Euclidean distance from the target row. The missing value is then calculated as a weighted mean from these neighboring rows.\n",
    "\n",
    "Below is an implementation for k = 2. Because we know our data is sorted we can code this much more efficiently. However, this isn't a general implementation. We also ignore the possibility that both of the closest neighbors can be on the same side to reduce the complexity of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cust = np.array(y)\n",
    "for ind in indices:\n",
    "    low1 = ind-1\n",
    "    while low1 in indices:\n",
    "        low1 = low1 - 1\n",
    "    high1 = ind + 1\n",
    "    while high1 in indices:\n",
    "        high1 = high1 + 1\n",
    "    d1 = 1/(ind - low1)\n",
    "    d2 = 1/(high1 - ind)\n",
    "    y_cust[ind] = (d1*y_cust[low1]+d2*y_cust[high1])/(d1+d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12345326331035167"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y_cust)\n",
    "mse(y_cust,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Comparison and Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Pima Indians Diabetes database for our example use case. This is an example of a MAR dataset but we will treat it as MCAR to make the best out of what we have. You can download the data from - https://www.kaggle.com/kumargh/pimaindiansdiabetescsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('pima-indians-diabetes.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 0. Number of times pregnant\n",
    " 1. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    " 2. Diastolic blood pressure (mm Hg)\n",
    " 3. Triceps skin fold thickness (mm)\n",
    " 4. 2-Hour serum insulin (mu U/ml)\n",
    " 5. Body mass index (weight in kg/(height in m)^2)\n",
    " 6. Diabetes pedigree function\n",
    " 7. Age (years)\n",
    " 8. Class variable (0 or 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, a person cannot have triceps sking fold thickness as 0 mm. This is a missing value and we need to replace 0 with NaN to let our algorithms know that it's a missing value.\n",
    "\n",
    "By reading the descriptions we can be sure that columns 1,2,3,4,5,6 and 7 cannot have zero values. As such, we will mark 0s as missing. \n",
    "\n",
    "Also, imputing functions work better with scaled features so we will use MinMaxScaler to scale every feature between 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      5\n",
       "2     35\n",
       "3    227\n",
       "4    374\n",
       "5     11\n",
       "6      0\n",
       "7      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[[1,2,3,4,5,6,7]] == 0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame(data=MinMaxScaler().fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "df[[1,2,3,4,5,6,7]] = df[[1,2,3,4,5,6,7]].replace(0,float('NaN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.743719</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500745</td>\n",
       "      <td>0.234415</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.427136</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.116567</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.919598</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.347243</td>\n",
       "      <td>0.253629</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.447236</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.418778</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688442</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.353535</td>\n",
       "      <td>0.198582</td>\n",
       "      <td>0.642325</td>\n",
       "      <td>0.943638</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.352941  0.743719  0.590164  0.353535       NaN  0.500745  0.234415   \n",
       "1  0.058824  0.427136  0.540984  0.292929       NaN  0.396423  0.116567   \n",
       "2  0.470588  0.919598  0.524590       NaN       NaN  0.347243  0.253629   \n",
       "3  0.058824  0.447236  0.540984  0.232323  0.111111  0.418778  0.038002   \n",
       "4  0.000000  0.688442  0.327869  0.353535  0.198582  0.642325  0.943638   \n",
       "\n",
       "          7    8  \n",
       "0  0.483333  1.0  \n",
       "1  0.166667  0.0  \n",
       "2  0.183333  1.0  \n",
       "3       NaN  0.0  \n",
       "4  0.200000  1.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fancyimpute offers many different forms of imputation methods, however, we are only comparing the four mentioned below. You can read about all of these at https://pypi.org/project/fancyimpute/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compare Logistic Regression using four different imputation methods:\n",
    " - KNN\n",
    " - Mean\n",
    " - IterativeImputer\n",
    " - SoftImpute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first construct the dataframe for the bottom three because for KNN we need to find the optimum value of the hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Max Singular Value of X_init = 31.698472\n",
      "[SoftImpute] Iter 1: observed MAE=0.017449 rank=9\n",
      "[SoftImpute] Iter 2: observed MAE=0.017573 rank=9\n",
      "[SoftImpute] Iter 3: observed MAE=0.017683 rank=9\n",
      "[SoftImpute] Iter 4: observed MAE=0.017781 rank=9\n",
      "[SoftImpute] Iter 5: observed MAE=0.017869 rank=9\n",
      "[SoftImpute] Iter 6: observed MAE=0.017944 rank=9\n",
      "[SoftImpute] Iter 7: observed MAE=0.018009 rank=9\n",
      "[SoftImpute] Iter 8: observed MAE=0.018064 rank=9\n",
      "[SoftImpute] Iter 9: observed MAE=0.018111 rank=9\n",
      "[SoftImpute] Iter 10: observed MAE=0.018151 rank=9\n",
      "[SoftImpute] Iter 11: observed MAE=0.018186 rank=9\n",
      "[SoftImpute] Iter 12: observed MAE=0.018216 rank=9\n",
      "[SoftImpute] Iter 13: observed MAE=0.018242 rank=9\n",
      "[SoftImpute] Iter 14: observed MAE=0.018265 rank=9\n",
      "[SoftImpute] Iter 15: observed MAE=0.018285 rank=9\n",
      "[SoftImpute] Iter 16: observed MAE=0.018301 rank=9\n",
      "[SoftImpute] Iter 17: observed MAE=0.018315 rank=9\n",
      "[SoftImpute] Iter 18: observed MAE=0.018326 rank=9\n",
      "[SoftImpute] Iter 19: observed MAE=0.018337 rank=9\n",
      "[SoftImpute] Iter 20: observed MAE=0.018346 rank=9\n",
      "[SoftImpute] Iter 21: observed MAE=0.018353 rank=9\n",
      "[SoftImpute] Iter 22: observed MAE=0.018360 rank=9\n",
      "[SoftImpute] Iter 23: observed MAE=0.018366 rank=9\n",
      "[SoftImpute] Iter 24: observed MAE=0.018371 rank=9\n",
      "[SoftImpute] Iter 25: observed MAE=0.018375 rank=9\n",
      "[SoftImpute] Iter 26: observed MAE=0.018379 rank=9\n",
      "[SoftImpute] Iter 27: observed MAE=0.018382 rank=9\n",
      "[SoftImpute] Iter 28: observed MAE=0.018385 rank=9\n",
      "[SoftImpute] Iter 29: observed MAE=0.018387 rank=9\n",
      "[SoftImpute] Iter 30: observed MAE=0.018389 rank=9\n",
      "[SoftImpute] Iter 31: observed MAE=0.018391 rank=9\n",
      "[SoftImpute] Iter 32: observed MAE=0.018392 rank=9\n",
      "[SoftImpute] Iter 33: observed MAE=0.018393 rank=9\n",
      "[SoftImpute] Iter 34: observed MAE=0.018394 rank=9\n",
      "[SoftImpute] Iter 35: observed MAE=0.018395 rank=9\n",
      "[SoftImpute] Iter 36: observed MAE=0.018396 rank=9\n",
      "[SoftImpute] Iter 37: observed MAE=0.018397 rank=9\n",
      "[SoftImpute] Iter 38: observed MAE=0.018398 rank=9\n",
      "[SoftImpute] Iter 39: observed MAE=0.018398 rank=9\n",
      "[SoftImpute] Iter 40: observed MAE=0.018399 rank=9\n",
      "[SoftImpute] Stopped after iteration 40 for lambda=0.633969\n"
     ]
    }
   ],
   "source": [
    "df_mean=pd.DataFrame(data=fancyimpute.SimpleFill().fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "df_iterative=pd.DataFrame(data=fancyimpute.IterativeImputer().fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "df_soft=pd.DataFrame(data=fancyimpute.SoftImpute().fit_transform(df.values), columns=df.columns, index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "validation_split = 0.8\n",
    "input_columns = [0,1,2,3,4,5,6,7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3d57fbddc34a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogisticRegr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_mean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmean_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogisticRegr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_mean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmean_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_mean' is not defined"
     ]
    }
   ],
   "source": [
    "logisticRegr.fit(df_mean[:int(len(df)*validation_split)][input_columns], df[:int(len(df)*validation_split)][8].values )\n",
    "mean_score = logisticRegr.score(df_mean[int(len(df)*validation_split):][input_columns], df[int(len(df)*validation_split):][8].values )\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "logisticRegr.fit(df_iterative[:int(len(df)*validation_split)][input_columns], df[:int(len(df)*validation_split)][8].values )\n",
    "iter_score = logisticRegr.score(df_iterative[int(len(df)*validation_split):][input_columns], df[int(len(df)*validation_split):][8].values )\n",
    "iter_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "logisticRegr.fit(df_soft[:int(len(df)*validation_split)][input_columns], df[:int(len(df)*validation_split)][8].values )\n",
    "soft_score = logisticRegr.score(df_soft[int(len(df)*validation_split):][input_columns], df[int(len(df)*validation_split):][8].values )\n",
    "soft_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn = []\n",
    "\n",
    "for k in range(2,30):\n",
    "    df_knn=pd.DataFrame(data=fancyimpute.KNN(k=k).fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "    logisticRegr.fit(df_knn[:int(len(df)*validation_split)][input_columns], df[:int(len(df)*validation_split)][8].values )\n",
    "    results_knn.append(logisticRegr.score(df_knn[int(len(df)*validation_split):][input_columns], df[int(len(df)*validation_split):][8].values ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarising the results:\n",
    " - Mean Imputation - 75.97%\n",
    " - Iterative Imputer - 77.27%\n",
    " - Soft Imputer - 77.27%\n",
    " - KNN Imputation - 80.52%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often claimed that mean imputation is just as good as the fancier methods such as KNN when used in conjunction with more complicated models. To test it, we build a simple neural network and train it with mean imputed data and compare results with KNN imputed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614 samples, validate on 154 samples\n",
      "Epoch 1/400\n",
      "614/614 [==============================] - 0s 544us/step - loss: 0.6763 - acc: 0.6531 - val_loss: 0.6724 - val_acc: 0.6429\n",
      "Epoch 2/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.6682 - acc: 0.6531 - val_loss: 0.6665 - val_acc: 0.6429\n",
      "Epoch 3/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6633 - acc: 0.6531 - val_loss: 0.6635 - val_acc: 0.6429\n",
      "Epoch 4/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.6605 - acc: 0.6531 - val_loss: 0.6602 - val_acc: 0.6429\n",
      "Epoch 5/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.6573 - acc: 0.6531 - val_loss: 0.6578 - val_acc: 0.6429\n",
      "Epoch 6/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.6547 - acc: 0.6531 - val_loss: 0.6560 - val_acc: 0.6429\n",
      "Epoch 7/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.6531 - acc: 0.6531 - val_loss: 0.6545 - val_acc: 0.6429\n",
      "Epoch 8/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.6515 - acc: 0.6531 - val_loss: 0.6528 - val_acc: 0.6429\n",
      "Epoch 9/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.6499 - acc: 0.6531 - val_loss: 0.6515 - val_acc: 0.6429\n",
      "Epoch 10/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6484 - acc: 0.6531 - val_loss: 0.6503 - val_acc: 0.6429\n",
      "Epoch 11/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6473 - acc: 0.6531 - val_loss: 0.6492 - val_acc: 0.6429\n",
      "Epoch 12/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.6456 - acc: 0.6531 - val_loss: 0.6479 - val_acc: 0.6429\n",
      "Epoch 13/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.6441 - acc: 0.6531 - val_loss: 0.6464 - val_acc: 0.6429\n",
      "Epoch 14/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.6426 - acc: 0.6531 - val_loss: 0.6453 - val_acc: 0.6429\n",
      "Epoch 15/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.6412 - acc: 0.6531 - val_loss: 0.6438 - val_acc: 0.6429\n",
      "Epoch 16/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.6398 - acc: 0.6531 - val_loss: 0.6424 - val_acc: 0.6429\n",
      "Epoch 17/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.6382 - acc: 0.6531 - val_loss: 0.6406 - val_acc: 0.6429\n",
      "Epoch 18/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.6366 - acc: 0.6531 - val_loss: 0.6390 - val_acc: 0.6429\n",
      "Epoch 19/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6349 - acc: 0.6531 - val_loss: 0.6371 - val_acc: 0.6429\n",
      "Epoch 20/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.6329 - acc: 0.6531 - val_loss: 0.6360 - val_acc: 0.6429\n",
      "Epoch 21/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.6314 - acc: 0.6531 - val_loss: 0.6340 - val_acc: 0.6429\n",
      "Epoch 22/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6298 - acc: 0.6531 - val_loss: 0.6323 - val_acc: 0.6429\n",
      "Epoch 23/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.6276 - acc: 0.6531 - val_loss: 0.6307 - val_acc: 0.6429\n",
      "Epoch 24/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.6260 - acc: 0.6531 - val_loss: 0.6292 - val_acc: 0.6429\n",
      "Epoch 25/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.6238 - acc: 0.6531 - val_loss: 0.6264 - val_acc: 0.6429\n",
      "Epoch 26/400\n",
      "614/614 [==============================] - 0s 24us/step - loss: 0.6218 - acc: 0.6531 - val_loss: 0.6245 - val_acc: 0.6429\n",
      "Epoch 27/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.6193 - acc: 0.6531 - val_loss: 0.6230 - val_acc: 0.6429\n",
      "Epoch 28/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.6171 - acc: 0.6531 - val_loss: 0.6199 - val_acc: 0.6429\n",
      "Epoch 29/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.6147 - acc: 0.6531 - val_loss: 0.6178 - val_acc: 0.6429\n",
      "Epoch 30/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.6125 - acc: 0.6531 - val_loss: 0.6163 - val_acc: 0.6429\n",
      "Epoch 31/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.6099 - acc: 0.6547 - val_loss: 0.6139 - val_acc: 0.6429\n",
      "Epoch 32/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.6071 - acc: 0.6531 - val_loss: 0.6095 - val_acc: 0.6429\n",
      "Epoch 33/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.6045 - acc: 0.6564 - val_loss: 0.6091 - val_acc: 0.6429\n",
      "Epoch 34/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.6021 - acc: 0.6580 - val_loss: 0.6071 - val_acc: 0.6494\n",
      "Epoch 35/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.5995 - acc: 0.6596 - val_loss: 0.6040 - val_acc: 0.6429\n",
      "Epoch 36/400\n",
      "614/614 [==============================] - 0s 26us/step - loss: 0.5966 - acc: 0.6661 - val_loss: 0.5994 - val_acc: 0.6364\n",
      "Epoch 37/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.5939 - acc: 0.6792 - val_loss: 0.5985 - val_acc: 0.6364\n",
      "Epoch 38/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.5916 - acc: 0.6759 - val_loss: 0.5968 - val_acc: 0.6364\n",
      "Epoch 39/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.5890 - acc: 0.6906 - val_loss: 0.5937 - val_acc: 0.6299\n",
      "Epoch 40/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.5862 - acc: 0.6873 - val_loss: 0.5892 - val_acc: 0.6429\n",
      "Epoch 41/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.5838 - acc: 0.7020 - val_loss: 0.5871 - val_acc: 0.6364\n",
      "Epoch 42/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.5806 - acc: 0.7003 - val_loss: 0.5838 - val_acc: 0.6558\n",
      "Epoch 43/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.5777 - acc: 0.7182 - val_loss: 0.5851 - val_acc: 0.6429\n",
      "Epoch 44/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.5744 - acc: 0.7020 - val_loss: 0.5785 - val_acc: 0.6688\n",
      "Epoch 45/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.5707 - acc: 0.7182 - val_loss: 0.5762 - val_acc: 0.6753\n",
      "Epoch 46/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.5672 - acc: 0.7182 - val_loss: 0.5733 - val_acc: 0.6818\n",
      "Epoch 47/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.5627 - acc: 0.7199 - val_loss: 0.5694 - val_acc: 0.6948\n",
      "Epoch 48/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.5580 - acc: 0.7182 - val_loss: 0.5658 - val_acc: 0.7143\n",
      "Epoch 49/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.5551 - acc: 0.7329 - val_loss: 0.5637 - val_acc: 0.6948\n",
      "Epoch 50/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.5511 - acc: 0.7264 - val_loss: 0.5596 - val_acc: 0.6948\n",
      "Epoch 51/400\n",
      "614/614 [==============================] - 0s 26us/step - loss: 0.5477 - acc: 0.7313 - val_loss: 0.5581 - val_acc: 0.6948\n",
      "Epoch 52/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.5444 - acc: 0.7280 - val_loss: 0.5551 - val_acc: 0.6948\n",
      "Epoch 53/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.5416 - acc: 0.7313 - val_loss: 0.5494 - val_acc: 0.7078\n",
      "Epoch 54/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.5394 - acc: 0.7492 - val_loss: 0.5470 - val_acc: 0.7078\n",
      "Epoch 55/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.5359 - acc: 0.7459 - val_loss: 0.5404 - val_acc: 0.7597\n",
      "Epoch 56/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.5338 - acc: 0.7541 - val_loss: 0.5379 - val_acc: 0.7597\n",
      "Epoch 57/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.5322 - acc: 0.7557 - val_loss: 0.5356 - val_acc: 0.7532\n",
      "Epoch 58/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.5284 - acc: 0.7655 - val_loss: 0.5354 - val_acc: 0.7338\n",
      "Epoch 59/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.5261 - acc: 0.7557 - val_loss: 0.5301 - val_acc: 0.7597\n",
      "Epoch 60/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.5245 - acc: 0.7622 - val_loss: 0.5292 - val_acc: 0.7468\n",
      "Epoch 61/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 52us/step - loss: 0.5214 - acc: 0.7590 - val_loss: 0.5274 - val_acc: 0.7403\n",
      "Epoch 62/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.5196 - acc: 0.7590 - val_loss: 0.5237 - val_acc: 0.7532\n",
      "Epoch 63/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.5179 - acc: 0.7573 - val_loss: 0.5245 - val_acc: 0.7403\n",
      "Epoch 64/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.5150 - acc: 0.7557 - val_loss: 0.5288 - val_acc: 0.7208\n",
      "Epoch 65/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.5140 - acc: 0.7622 - val_loss: 0.5250 - val_acc: 0.7338\n",
      "Epoch 66/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.5110 - acc: 0.7606 - val_loss: 0.5163 - val_acc: 0.7597\n",
      "Epoch 67/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.5095 - acc: 0.7687 - val_loss: 0.5204 - val_acc: 0.7338\n",
      "Epoch 68/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.5072 - acc: 0.7622 - val_loss: 0.5180 - val_acc: 0.7468\n",
      "Epoch 69/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.5047 - acc: 0.7638 - val_loss: 0.5115 - val_acc: 0.7468\n",
      "Epoch 70/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.5036 - acc: 0.7655 - val_loss: 0.5092 - val_acc: 0.7532\n",
      "Epoch 71/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.5010 - acc: 0.7687 - val_loss: 0.5073 - val_acc: 0.7532\n",
      "Epoch 72/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.5000 - acc: 0.7671 - val_loss: 0.5118 - val_acc: 0.7403\n",
      "Epoch 73/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4991 - acc: 0.7638 - val_loss: 0.5146 - val_acc: 0.7338\n",
      "Epoch 74/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4983 - acc: 0.7655 - val_loss: 0.5064 - val_acc: 0.7532\n",
      "Epoch 75/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4958 - acc: 0.7638 - val_loss: 0.5010 - val_acc: 0.7597\n",
      "Epoch 76/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4949 - acc: 0.7671 - val_loss: 0.4985 - val_acc: 0.7662\n",
      "Epoch 77/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4934 - acc: 0.7687 - val_loss: 0.4998 - val_acc: 0.7662\n",
      "Epoch 78/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4923 - acc: 0.7704 - val_loss: 0.4982 - val_acc: 0.7532\n",
      "Epoch 79/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4923 - acc: 0.7687 - val_loss: 0.4962 - val_acc: 0.7597\n",
      "Epoch 80/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4885 - acc: 0.7704 - val_loss: 0.5008 - val_acc: 0.7532\n",
      "Epoch 81/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4891 - acc: 0.7687 - val_loss: 0.4924 - val_acc: 0.7597\n",
      "Epoch 82/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4871 - acc: 0.7704 - val_loss: 0.5074 - val_acc: 0.7403\n",
      "Epoch 83/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4882 - acc: 0.7720 - val_loss: 0.4934 - val_acc: 0.7532\n",
      "Epoch 84/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4866 - acc: 0.7638 - val_loss: 0.4903 - val_acc: 0.7597\n",
      "Epoch 85/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4841 - acc: 0.7736 - val_loss: 0.4899 - val_acc: 0.7532\n",
      "Epoch 86/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4830 - acc: 0.7638 - val_loss: 0.4865 - val_acc: 0.7662\n",
      "Epoch 87/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4833 - acc: 0.7671 - val_loss: 0.4875 - val_acc: 0.7662\n",
      "Epoch 88/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4819 - acc: 0.7720 - val_loss: 0.4844 - val_acc: 0.7662\n",
      "Epoch 89/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4814 - acc: 0.7671 - val_loss: 0.4875 - val_acc: 0.7597\n",
      "Epoch 90/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4808 - acc: 0.7752 - val_loss: 0.4834 - val_acc: 0.7662\n",
      "Epoch 91/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4797 - acc: 0.7704 - val_loss: 0.4827 - val_acc: 0.7662\n",
      "Epoch 92/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4782 - acc: 0.7687 - val_loss: 0.4879 - val_acc: 0.7597\n",
      "Epoch 93/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4790 - acc: 0.7687 - val_loss: 0.4852 - val_acc: 0.7597\n",
      "Epoch 94/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4771 - acc: 0.7606 - val_loss: 0.4854 - val_acc: 0.7597\n",
      "Epoch 95/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4769 - acc: 0.7720 - val_loss: 0.4860 - val_acc: 0.7532\n",
      "Epoch 96/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4768 - acc: 0.7736 - val_loss: 0.4859 - val_acc: 0.7532\n",
      "Epoch 97/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4747 - acc: 0.7687 - val_loss: 0.4828 - val_acc: 0.7597\n",
      "Epoch 98/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4745 - acc: 0.7736 - val_loss: 0.4810 - val_acc: 0.7597\n",
      "Epoch 99/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4743 - acc: 0.7736 - val_loss: 0.4790 - val_acc: 0.7662\n",
      "Epoch 100/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4743 - acc: 0.7736 - val_loss: 0.4809 - val_acc: 0.7597\n",
      "Epoch 101/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4728 - acc: 0.7752 - val_loss: 0.4782 - val_acc: 0.7597\n",
      "Epoch 102/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4728 - acc: 0.7752 - val_loss: 0.4778 - val_acc: 0.7597\n",
      "Epoch 103/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4722 - acc: 0.7687 - val_loss: 0.4753 - val_acc: 0.7727\n",
      "Epoch 104/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4716 - acc: 0.7736 - val_loss: 0.4741 - val_acc: 0.7727\n",
      "Epoch 105/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4703 - acc: 0.7622 - val_loss: 0.4741 - val_acc: 0.7792\n",
      "Epoch 106/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4693 - acc: 0.7752 - val_loss: 0.4731 - val_acc: 0.7727\n",
      "Epoch 107/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4698 - acc: 0.7590 - val_loss: 0.4830 - val_acc: 0.7597\n",
      "Epoch 108/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4697 - acc: 0.7704 - val_loss: 0.4728 - val_acc: 0.7597\n",
      "Epoch 109/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4686 - acc: 0.7720 - val_loss: 0.4734 - val_acc: 0.7597\n",
      "Epoch 110/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4687 - acc: 0.7720 - val_loss: 0.4733 - val_acc: 0.7597\n",
      "Epoch 111/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4678 - acc: 0.7785 - val_loss: 0.4814 - val_acc: 0.7597\n",
      "Epoch 112/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4676 - acc: 0.7687 - val_loss: 0.4788 - val_acc: 0.7597\n",
      "Epoch 113/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4667 - acc: 0.7752 - val_loss: 0.4716 - val_acc: 0.7727\n",
      "Epoch 114/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4666 - acc: 0.7704 - val_loss: 0.4709 - val_acc: 0.7597\n",
      "Epoch 115/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4662 - acc: 0.7769 - val_loss: 0.4707 - val_acc: 0.7597\n",
      "Epoch 116/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4656 - acc: 0.7704 - val_loss: 0.4809 - val_acc: 0.7662\n",
      "Epoch 117/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4652 - acc: 0.7752 - val_loss: 0.4714 - val_acc: 0.7597\n",
      "Epoch 118/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4651 - acc: 0.7769 - val_loss: 0.4701 - val_acc: 0.7597\n",
      "Epoch 119/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4647 - acc: 0.7736 - val_loss: 0.4691 - val_acc: 0.7597\n",
      "Epoch 120/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4652 - acc: 0.7687 - val_loss: 0.4692 - val_acc: 0.7597\n",
      "Epoch 121/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 36us/step - loss: 0.4653 - acc: 0.7720 - val_loss: 0.4704 - val_acc: 0.7597\n",
      "Epoch 122/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4642 - acc: 0.7720 - val_loss: 0.4682 - val_acc: 0.7662\n",
      "Epoch 123/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4605 - acc: 0.7704 - val_loss: 0.4724 - val_acc: 0.7597\n",
      "Epoch 124/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4626 - acc: 0.7801 - val_loss: 0.4683 - val_acc: 0.7662\n",
      "Epoch 125/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4624 - acc: 0.7720 - val_loss: 0.4705 - val_acc: 0.7662\n",
      "Epoch 126/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4605 - acc: 0.7704 - val_loss: 0.4715 - val_acc: 0.7922\n",
      "Epoch 127/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4629 - acc: 0.7687 - val_loss: 0.4687 - val_acc: 0.7662\n",
      "Epoch 128/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4616 - acc: 0.7769 - val_loss: 0.4685 - val_acc: 0.7597\n",
      "Epoch 129/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4623 - acc: 0.7704 - val_loss: 0.4670 - val_acc: 0.7597\n",
      "Epoch 130/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4606 - acc: 0.7720 - val_loss: 0.4701 - val_acc: 0.7597\n",
      "Epoch 131/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4611 - acc: 0.7720 - val_loss: 0.4704 - val_acc: 0.7597\n",
      "Epoch 132/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4606 - acc: 0.7769 - val_loss: 0.4722 - val_acc: 0.7662\n",
      "Epoch 133/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4594 - acc: 0.7850 - val_loss: 0.4725 - val_acc: 0.7597\n",
      "Epoch 134/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4597 - acc: 0.7736 - val_loss: 0.4662 - val_acc: 0.7597\n",
      "Epoch 135/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4571 - acc: 0.7736 - val_loss: 0.4664 - val_acc: 0.7662\n",
      "Epoch 136/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4587 - acc: 0.7736 - val_loss: 0.4657 - val_acc: 0.7662\n",
      "Epoch 137/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4581 - acc: 0.7736 - val_loss: 0.4783 - val_acc: 0.7597\n",
      "Epoch 138/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4595 - acc: 0.7801 - val_loss: 0.4749 - val_acc: 0.7532\n",
      "Epoch 139/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4578 - acc: 0.7736 - val_loss: 0.4662 - val_acc: 0.7597\n",
      "Epoch 140/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4584 - acc: 0.7736 - val_loss: 0.4652 - val_acc: 0.7597\n",
      "Epoch 141/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4566 - acc: 0.7752 - val_loss: 0.4643 - val_acc: 0.7727\n",
      "Epoch 142/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4584 - acc: 0.7736 - val_loss: 0.4639 - val_acc: 0.7662\n",
      "Epoch 143/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4554 - acc: 0.7736 - val_loss: 0.4652 - val_acc: 0.7662\n",
      "Epoch 144/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4573 - acc: 0.7671 - val_loss: 0.4679 - val_acc: 0.7662\n",
      "Epoch 145/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4556 - acc: 0.7638 - val_loss: 0.4627 - val_acc: 0.7662\n",
      "Epoch 146/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4544 - acc: 0.7818 - val_loss: 0.4638 - val_acc: 0.7662\n",
      "Epoch 147/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4556 - acc: 0.7720 - val_loss: 0.4717 - val_acc: 0.7662\n",
      "Epoch 148/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4561 - acc: 0.7720 - val_loss: 0.4715 - val_acc: 0.7597\n",
      "Epoch 149/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4544 - acc: 0.7736 - val_loss: 0.4638 - val_acc: 0.7597\n",
      "Epoch 150/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4551 - acc: 0.7736 - val_loss: 0.4660 - val_acc: 0.7597\n",
      "Epoch 151/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4540 - acc: 0.7769 - val_loss: 0.4652 - val_acc: 0.7597\n",
      "Epoch 152/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4551 - acc: 0.7801 - val_loss: 0.4660 - val_acc: 0.7597\n",
      "Epoch 153/400\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.4538 - acc: 0.7785 - val_loss: 0.4707 - val_acc: 0.7597\n",
      "Epoch 154/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4540 - acc: 0.7704 - val_loss: 0.4631 - val_acc: 0.7662\n",
      "Epoch 155/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4535 - acc: 0.7769 - val_loss: 0.4620 - val_acc: 0.7662\n",
      "Epoch 156/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4540 - acc: 0.7671 - val_loss: 0.4626 - val_acc: 0.7662\n",
      "Epoch 157/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4538 - acc: 0.7736 - val_loss: 0.4657 - val_acc: 0.7597\n",
      "Epoch 158/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4532 - acc: 0.7720 - val_loss: 0.4645 - val_acc: 0.7597\n",
      "Epoch 159/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4539 - acc: 0.7769 - val_loss: 0.4641 - val_acc: 0.7597\n",
      "Epoch 160/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4527 - acc: 0.7883 - val_loss: 0.4626 - val_acc: 0.7727\n",
      "Epoch 161/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4530 - acc: 0.7769 - val_loss: 0.4623 - val_acc: 0.7662\n",
      "Epoch 162/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4522 - acc: 0.7801 - val_loss: 0.4630 - val_acc: 0.7532\n",
      "Epoch 163/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4531 - acc: 0.7736 - val_loss: 0.4675 - val_acc: 0.7532\n",
      "Epoch 164/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4520 - acc: 0.7801 - val_loss: 0.4679 - val_acc: 0.7532\n",
      "Epoch 165/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4509 - acc: 0.7769 - val_loss: 0.4647 - val_acc: 0.7532\n",
      "Epoch 166/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4518 - acc: 0.7834 - val_loss: 0.4631 - val_acc: 0.7662\n",
      "Epoch 167/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4510 - acc: 0.7866 - val_loss: 0.4691 - val_acc: 0.7532\n",
      "Epoch 168/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4506 - acc: 0.7866 - val_loss: 0.4734 - val_acc: 0.7532\n",
      "Epoch 169/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4516 - acc: 0.7752 - val_loss: 0.4631 - val_acc: 0.7857\n",
      "Epoch 170/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4518 - acc: 0.7785 - val_loss: 0.4636 - val_acc: 0.7662\n",
      "Epoch 171/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4475 - acc: 0.7752 - val_loss: 0.4810 - val_acc: 0.7597\n",
      "Epoch 172/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4505 - acc: 0.7801 - val_loss: 0.4828 - val_acc: 0.7597\n",
      "Epoch 173/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4520 - acc: 0.7720 - val_loss: 0.4638 - val_acc: 0.7662\n",
      "Epoch 174/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4512 - acc: 0.7801 - val_loss: 0.4632 - val_acc: 0.7662\n",
      "Epoch 175/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.4488 - acc: 0.7720 - val_loss: 0.4615 - val_acc: 0.7662\n",
      "Epoch 176/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4494 - acc: 0.7704 - val_loss: 0.4659 - val_acc: 0.7662\n",
      "Epoch 177/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4482 - acc: 0.7834 - val_loss: 0.4647 - val_acc: 0.7792\n",
      "Epoch 178/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4493 - acc: 0.7785 - val_loss: 0.4631 - val_acc: 0.7727\n",
      "Epoch 179/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4500 - acc: 0.7655 - val_loss: 0.4617 - val_acc: 0.7662\n",
      "Epoch 180/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4495 - acc: 0.7720 - val_loss: 0.4623 - val_acc: 0.7662\n",
      "Epoch 181/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 39us/step - loss: 0.4487 - acc: 0.7720 - val_loss: 0.4644 - val_acc: 0.7792\n",
      "Epoch 182/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.4485 - acc: 0.7769 - val_loss: 0.4637 - val_acc: 0.7597\n",
      "Epoch 183/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4490 - acc: 0.7850 - val_loss: 0.4745 - val_acc: 0.7597\n",
      "Epoch 184/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4478 - acc: 0.7736 - val_loss: 0.4655 - val_acc: 0.7792\n",
      "Epoch 185/400\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4474 - acc: 0.7818 - val_loss: 0.4632 - val_acc: 0.7662\n",
      "Epoch 186/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4475 - acc: 0.7801 - val_loss: 0.4653 - val_acc: 0.7857\n",
      "Epoch 187/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4504 - acc: 0.7752 - val_loss: 0.4689 - val_acc: 0.7468\n",
      "Epoch 188/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4479 - acc: 0.7769 - val_loss: 0.4666 - val_acc: 0.7532\n",
      "Epoch 189/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4474 - acc: 0.7801 - val_loss: 0.4635 - val_acc: 0.7662\n",
      "Epoch 190/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4470 - acc: 0.7769 - val_loss: 0.4654 - val_acc: 0.7532\n",
      "Epoch 191/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4475 - acc: 0.7818 - val_loss: 0.4671 - val_acc: 0.7727\n",
      "Epoch 192/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4474 - acc: 0.7785 - val_loss: 0.4661 - val_acc: 0.7532\n",
      "Epoch 193/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4476 - acc: 0.7801 - val_loss: 0.4651 - val_acc: 0.7727\n",
      "Epoch 194/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4470 - acc: 0.7769 - val_loss: 0.4688 - val_acc: 0.7532\n",
      "Epoch 195/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4457 - acc: 0.7834 - val_loss: 0.4632 - val_acc: 0.7662\n",
      "Epoch 196/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4469 - acc: 0.7720 - val_loss: 0.4670 - val_acc: 0.7468\n",
      "Epoch 197/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4463 - acc: 0.7785 - val_loss: 0.4727 - val_acc: 0.7532\n",
      "Epoch 198/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4471 - acc: 0.7834 - val_loss: 0.4656 - val_acc: 0.7597\n",
      "Epoch 199/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4463 - acc: 0.7769 - val_loss: 0.4663 - val_acc: 0.7597\n",
      "Epoch 200/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4440 - acc: 0.7818 - val_loss: 0.4679 - val_acc: 0.7597\n",
      "Epoch 201/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4467 - acc: 0.7818 - val_loss: 0.4659 - val_acc: 0.7662\n",
      "Epoch 202/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4457 - acc: 0.7834 - val_loss: 0.4645 - val_acc: 0.7662\n",
      "Epoch 203/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4447 - acc: 0.7769 - val_loss: 0.4790 - val_acc: 0.7597\n",
      "Epoch 204/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4474 - acc: 0.7752 - val_loss: 0.4663 - val_acc: 0.7468\n",
      "Epoch 205/400\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.4717 - acc: 0.750 - 0s 44us/step - loss: 0.4459 - acc: 0.7769 - val_loss: 0.4649 - val_acc: 0.7597\n",
      "Epoch 206/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4446 - acc: 0.7866 - val_loss: 0.4774 - val_acc: 0.7597\n",
      "Epoch 207/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4453 - acc: 0.7834 - val_loss: 0.4647 - val_acc: 0.7468\n",
      "Epoch 208/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4455 - acc: 0.7818 - val_loss: 0.4654 - val_acc: 0.7662\n",
      "Epoch 209/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4437 - acc: 0.7801 - val_loss: 0.4656 - val_acc: 0.7597\n",
      "Epoch 210/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4452 - acc: 0.7866 - val_loss: 0.4666 - val_acc: 0.7403\n",
      "Epoch 211/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4439 - acc: 0.7850 - val_loss: 0.4648 - val_acc: 0.7597\n",
      "Epoch 212/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4471 - acc: 0.7720 - val_loss: 0.4701 - val_acc: 0.7468\n",
      "Epoch 213/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4436 - acc: 0.7785 - val_loss: 0.4646 - val_acc: 0.7532\n",
      "Epoch 214/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4458 - acc: 0.7736 - val_loss: 0.4676 - val_acc: 0.7468\n",
      "Epoch 215/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4428 - acc: 0.7818 - val_loss: 0.4782 - val_acc: 0.7597\n",
      "Epoch 216/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4451 - acc: 0.7834 - val_loss: 0.4662 - val_acc: 0.7597\n",
      "Epoch 217/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4445 - acc: 0.7866 - val_loss: 0.4665 - val_acc: 0.7792\n",
      "Epoch 218/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4447 - acc: 0.7785 - val_loss: 0.4687 - val_acc: 0.7857\n",
      "Epoch 219/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.4442 - acc: 0.7752 - val_loss: 0.4728 - val_acc: 0.7857\n",
      "Epoch 220/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4461 - acc: 0.7769 - val_loss: 0.4680 - val_acc: 0.7468\n",
      "Epoch 221/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4437 - acc: 0.7801 - val_loss: 0.4667 - val_acc: 0.7532\n",
      "Epoch 222/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4446 - acc: 0.7752 - val_loss: 0.4701 - val_acc: 0.7468\n",
      "Epoch 223/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4425 - acc: 0.7801 - val_loss: 0.4659 - val_acc: 0.7532\n",
      "Epoch 224/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4425 - acc: 0.7785 - val_loss: 0.4656 - val_acc: 0.7468\n",
      "Epoch 225/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4446 - acc: 0.7769 - val_loss: 0.4669 - val_acc: 0.7403\n",
      "Epoch 226/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4421 - acc: 0.7818 - val_loss: 0.4661 - val_acc: 0.7597\n",
      "Epoch 227/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4427 - acc: 0.7752 - val_loss: 0.4654 - val_acc: 0.7532\n",
      "Epoch 228/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4439 - acc: 0.7785 - val_loss: 0.4679 - val_acc: 0.7857\n",
      "Epoch 229/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4434 - acc: 0.7818 - val_loss: 0.4704 - val_acc: 0.7468\n",
      "Epoch 230/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4416 - acc: 0.7785 - val_loss: 0.4721 - val_acc: 0.7468\n",
      "Epoch 231/400\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4413 - acc: 0.7818 - val_loss: 0.4673 - val_acc: 0.7532\n",
      "Epoch 232/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4416 - acc: 0.7769 - val_loss: 0.4697 - val_acc: 0.7532\n",
      "Epoch 233/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4424 - acc: 0.7850 - val_loss: 0.4684 - val_acc: 0.7597\n",
      "Epoch 234/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4433 - acc: 0.7785 - val_loss: 0.4676 - val_acc: 0.7468\n",
      "Epoch 235/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4414 - acc: 0.7883 - val_loss: 0.4714 - val_acc: 0.7857\n",
      "Epoch 236/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4413 - acc: 0.7850 - val_loss: 0.4734 - val_acc: 0.7468\n",
      "Epoch 237/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4405 - acc: 0.7834 - val_loss: 0.4723 - val_acc: 0.7532\n",
      "Epoch 238/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.4426 - acc: 0.7785 - val_loss: 0.4673 - val_acc: 0.7468\n",
      "Epoch 239/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4432 - acc: 0.7818 - val_loss: 0.4688 - val_acc: 0.7403\n",
      "Epoch 240/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4425 - acc: 0.7883 - val_loss: 0.4676 - val_acc: 0.7403\n",
      "Epoch 241/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 36us/step - loss: 0.4428 - acc: 0.7801 - val_loss: 0.4696 - val_acc: 0.7727\n",
      "Epoch 242/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4415 - acc: 0.7801 - val_loss: 0.4705 - val_acc: 0.7468\n",
      "Epoch 243/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4417 - acc: 0.7948 - val_loss: 0.4687 - val_acc: 0.7532\n",
      "Epoch 244/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4415 - acc: 0.7834 - val_loss: 0.4731 - val_acc: 0.7532\n",
      "Epoch 245/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.4396 - acc: 0.7915 - val_loss: 0.4696 - val_acc: 0.7727\n",
      "Epoch 246/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4423 - acc: 0.7801 - val_loss: 0.4676 - val_acc: 0.7338\n",
      "Epoch 247/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4405 - acc: 0.7883 - val_loss: 0.4660 - val_acc: 0.7468\n",
      "Epoch 248/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4405 - acc: 0.7752 - val_loss: 0.4676 - val_acc: 0.7468\n",
      "Epoch 249/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4409 - acc: 0.7818 - val_loss: 0.4782 - val_acc: 0.7532\n",
      "Epoch 250/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4406 - acc: 0.7818 - val_loss: 0.4675 - val_acc: 0.7403\n",
      "Epoch 251/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4397 - acc: 0.7850 - val_loss: 0.4701 - val_acc: 0.7468\n",
      "Epoch 252/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4396 - acc: 0.7932 - val_loss: 0.4692 - val_acc: 0.7403\n",
      "Epoch 253/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.4401 - acc: 0.7834 - val_loss: 0.4682 - val_acc: 0.7403\n",
      "Epoch 254/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4385 - acc: 0.7948 - val_loss: 0.4748 - val_acc: 0.7857\n",
      "Epoch 255/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4426 - acc: 0.7866 - val_loss: 0.4689 - val_acc: 0.7403\n",
      "Epoch 256/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4403 - acc: 0.7866 - val_loss: 0.4773 - val_acc: 0.7532\n",
      "Epoch 257/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4386 - acc: 0.7932 - val_loss: 0.4790 - val_acc: 0.7468\n",
      "Epoch 258/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4404 - acc: 0.7883 - val_loss: 0.4730 - val_acc: 0.7532\n",
      "Epoch 259/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4398 - acc: 0.7850 - val_loss: 0.4701 - val_acc: 0.7403\n",
      "Epoch 260/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.4408 - acc: 0.7883 - val_loss: 0.4722 - val_acc: 0.7468\n",
      "Epoch 261/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4395 - acc: 0.7899 - val_loss: 0.4759 - val_acc: 0.7532\n",
      "Epoch 262/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4399 - acc: 0.7866 - val_loss: 0.4758 - val_acc: 0.7532\n",
      "Epoch 263/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.4403 - acc: 0.7866 - val_loss: 0.4700 - val_acc: 0.7273\n",
      "Epoch 264/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4390 - acc: 0.7899 - val_loss: 0.4716 - val_acc: 0.7792\n",
      "Epoch 265/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.4381 - acc: 0.7932 - val_loss: 0.4779 - val_acc: 0.7532\n",
      "Epoch 266/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4418 - acc: 0.7915 - val_loss: 0.4699 - val_acc: 0.7403\n",
      "Epoch 267/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4391 - acc: 0.7899 - val_loss: 0.4722 - val_acc: 0.7662\n",
      "Epoch 268/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.4399 - acc: 0.7915 - val_loss: 0.4764 - val_acc: 0.7532\n",
      "Epoch 269/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4367 - acc: 0.7850 - val_loss: 0.4710 - val_acc: 0.7727\n",
      "Epoch 270/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4397 - acc: 0.7801 - val_loss: 0.4695 - val_acc: 0.7338\n",
      "Epoch 271/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4372 - acc: 0.7866 - val_loss: 0.4698 - val_acc: 0.7403\n",
      "Epoch 272/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4386 - acc: 0.7899 - val_loss: 0.4694 - val_acc: 0.7403\n",
      "Epoch 273/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4398 - acc: 0.7850 - val_loss: 0.4740 - val_acc: 0.7468\n",
      "Epoch 274/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4371 - acc: 0.7915 - val_loss: 0.4780 - val_acc: 0.7857\n",
      "Epoch 275/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4397 - acc: 0.7964 - val_loss: 0.4727 - val_acc: 0.7403\n",
      "Epoch 276/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4377 - acc: 0.7932 - val_loss: 0.4738 - val_acc: 0.7532\n",
      "Epoch 277/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4381 - acc: 0.7850 - val_loss: 0.4819 - val_acc: 0.7597\n",
      "Epoch 278/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4388 - acc: 0.7850 - val_loss: 0.4838 - val_acc: 0.7532\n",
      "Epoch 279/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4400 - acc: 0.7801 - val_loss: 0.4688 - val_acc: 0.7532\n",
      "Epoch 280/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4386 - acc: 0.7850 - val_loss: 0.4752 - val_acc: 0.7403\n",
      "Epoch 281/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.4394 - acc: 0.7801 - val_loss: 0.4694 - val_acc: 0.7468\n",
      "Epoch 282/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4365 - acc: 0.7948 - val_loss: 0.4802 - val_acc: 0.7532\n",
      "Epoch 283/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.4383 - acc: 0.7932 - val_loss: 0.4748 - val_acc: 0.7468\n",
      "Epoch 284/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4385 - acc: 0.7834 - val_loss: 0.4729 - val_acc: 0.7403\n",
      "Epoch 285/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4390 - acc: 0.7834 - val_loss: 0.4728 - val_acc: 0.7468\n",
      "Epoch 286/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4364 - acc: 0.7850 - val_loss: 0.4720 - val_acc: 0.7338\n",
      "Epoch 287/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4379 - acc: 0.7818 - val_loss: 0.4717 - val_acc: 0.7468\n",
      "Epoch 288/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4375 - acc: 0.7899 - val_loss: 0.4751 - val_acc: 0.7468\n",
      "Epoch 289/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4374 - acc: 0.7932 - val_loss: 0.4718 - val_acc: 0.7403\n",
      "Epoch 290/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4371 - acc: 0.7899 - val_loss: 0.4732 - val_acc: 0.7403\n",
      "Epoch 291/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4371 - acc: 0.7899 - val_loss: 0.4747 - val_acc: 0.7468\n",
      "Epoch 292/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4362 - acc: 0.7850 - val_loss: 0.4752 - val_acc: 0.7727\n",
      "Epoch 293/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4367 - acc: 0.7948 - val_loss: 0.4819 - val_acc: 0.7532\n",
      "Epoch 294/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4376 - acc: 0.7915 - val_loss: 0.4719 - val_acc: 0.7338\n",
      "Epoch 295/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4382 - acc: 0.7850 - val_loss: 0.4777 - val_acc: 0.7468\n",
      "Epoch 296/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4372 - acc: 0.7866 - val_loss: 0.4740 - val_acc: 0.7468\n",
      "Epoch 297/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4371 - acc: 0.7866 - val_loss: 0.4706 - val_acc: 0.7403\n",
      "Epoch 298/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4377 - acc: 0.7915 - val_loss: 0.4710 - val_acc: 0.7403\n",
      "Epoch 299/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4369 - acc: 0.7899 - val_loss: 0.4724 - val_acc: 0.7338\n",
      "Epoch 300/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4380 - acc: 0.7850 - val_loss: 0.4739 - val_acc: 0.7403\n",
      "Epoch 301/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 42us/step - loss: 0.4343 - acc: 0.7850 - val_loss: 0.4787 - val_acc: 0.7597\n",
      "Epoch 302/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4386 - acc: 0.7883 - val_loss: 0.4738 - val_acc: 0.7468\n",
      "Epoch 303/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4369 - acc: 0.7932 - val_loss: 0.4710 - val_acc: 0.7403\n",
      "Epoch 304/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4386 - acc: 0.7866 - val_loss: 0.4786 - val_acc: 0.7468\n",
      "Epoch 305/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4374 - acc: 0.7915 - val_loss: 0.4731 - val_acc: 0.7338\n",
      "Epoch 306/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4380 - acc: 0.7948 - val_loss: 0.4729 - val_acc: 0.7338\n",
      "Epoch 307/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4366 - acc: 0.7899 - val_loss: 0.4718 - val_acc: 0.7403\n",
      "Epoch 308/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4356 - acc: 0.8029 - val_loss: 0.4722 - val_acc: 0.7468\n",
      "Epoch 309/400\n",
      "614/614 [==============================] - 0s 24us/step - loss: 0.4375 - acc: 0.7866 - val_loss: 0.4767 - val_acc: 0.7468\n",
      "Epoch 310/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4353 - acc: 0.7801 - val_loss: 0.4754 - val_acc: 0.7468\n",
      "Epoch 311/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4366 - acc: 0.7899 - val_loss: 0.4730 - val_acc: 0.7403\n",
      "Epoch 312/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.4375 - acc: 0.7850 - val_loss: 0.4777 - val_acc: 0.7403\n",
      "Epoch 313/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4367 - acc: 0.7899 - val_loss: 0.4755 - val_acc: 0.7662\n",
      "Epoch 314/400\n",
      "614/614 [==============================] - 0s 26us/step - loss: 0.4378 - acc: 0.7785 - val_loss: 0.4833 - val_acc: 0.7532\n",
      "Epoch 315/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4382 - acc: 0.7899 - val_loss: 0.4715 - val_acc: 0.7403\n",
      "Epoch 316/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4356 - acc: 0.7915 - val_loss: 0.4715 - val_acc: 0.7468\n",
      "Epoch 317/400\n",
      "614/614 [==============================] - 0s 26us/step - loss: 0.4357 - acc: 0.7915 - val_loss: 0.4766 - val_acc: 0.7727\n",
      "Epoch 318/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4369 - acc: 0.7980 - val_loss: 0.4795 - val_acc: 0.7468\n",
      "Epoch 319/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4357 - acc: 0.7948 - val_loss: 0.4723 - val_acc: 0.7468\n",
      "Epoch 320/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.4361 - acc: 0.7915 - val_loss: 0.4744 - val_acc: 0.7338\n",
      "Epoch 321/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4371 - acc: 0.7932 - val_loss: 0.4795 - val_acc: 0.7468\n",
      "Epoch 322/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4354 - acc: 0.7932 - val_loss: 0.4748 - val_acc: 0.7338\n",
      "Epoch 323/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4362 - acc: 0.7850 - val_loss: 0.4818 - val_acc: 0.7468\n",
      "Epoch 324/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.4368 - acc: 0.7915 - val_loss: 0.4730 - val_acc: 0.7338\n",
      "Epoch 325/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4366 - acc: 0.7899 - val_loss: 0.4753 - val_acc: 0.7338\n",
      "Epoch 326/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4341 - acc: 0.7883 - val_loss: 0.4736 - val_acc: 0.7403\n",
      "Epoch 327/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4363 - acc: 0.7818 - val_loss: 0.4734 - val_acc: 0.7273\n",
      "Epoch 328/400\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4338 - acc: 0.7866 - val_loss: 0.4740 - val_acc: 0.7532\n",
      "Epoch 329/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4364 - acc: 0.7834 - val_loss: 0.4789 - val_acc: 0.7403\n",
      "Epoch 330/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4353 - acc: 0.7866 - val_loss: 0.4706 - val_acc: 0.7468\n",
      "Epoch 331/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4345 - acc: 0.7932 - val_loss: 0.4756 - val_acc: 0.7662\n",
      "Epoch 332/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.4751 - val_acc: 0.7338\n",
      "Epoch 333/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4348 - acc: 0.7850 - val_loss: 0.4778 - val_acc: 0.7403\n",
      "Epoch 334/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4361 - acc: 0.7915 - val_loss: 0.4769 - val_acc: 0.7338\n",
      "Epoch 335/400\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4332 - acc: 0.7834 - val_loss: 0.4730 - val_acc: 0.7597\n",
      "Epoch 336/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4375 - acc: 0.7883 - val_loss: 0.4727 - val_acc: 0.7338\n",
      "Epoch 337/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4347 - acc: 0.7899 - val_loss: 0.4758 - val_acc: 0.7338\n",
      "Epoch 338/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4350 - acc: 0.7883 - val_loss: 0.4720 - val_acc: 0.7403\n",
      "Epoch 339/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4358 - acc: 0.7818 - val_loss: 0.4744 - val_acc: 0.7338\n",
      "Epoch 340/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4339 - acc: 0.7948 - val_loss: 0.4750 - val_acc: 0.7338\n",
      "Epoch 341/400\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4336 - acc: 0.7883 - val_loss: 0.4803 - val_acc: 0.7468\n",
      "Epoch 342/400\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4347 - acc: 0.7964 - val_loss: 0.4882 - val_acc: 0.7468\n",
      "Epoch 343/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4374 - acc: 0.7932 - val_loss: 0.4720 - val_acc: 0.7403\n",
      "Epoch 344/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4349 - acc: 0.7915 - val_loss: 0.4748 - val_acc: 0.7403\n",
      "Epoch 345/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4349 - acc: 0.7866 - val_loss: 0.4769 - val_acc: 0.7338\n",
      "Epoch 346/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4332 - acc: 0.7915 - val_loss: 0.4737 - val_acc: 0.7338\n",
      "Epoch 347/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4341 - acc: 0.7866 - val_loss: 0.4872 - val_acc: 0.7532\n",
      "Epoch 348/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4367 - acc: 0.7915 - val_loss: 0.4760 - val_acc: 0.7338\n",
      "Epoch 349/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4339 - acc: 0.7948 - val_loss: 0.4779 - val_acc: 0.7403\n",
      "Epoch 350/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4348 - acc: 0.7948 - val_loss: 0.4729 - val_acc: 0.7403\n",
      "Epoch 351/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4335 - acc: 0.7818 - val_loss: 0.4757 - val_acc: 0.7338\n",
      "Epoch 352/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4338 - acc: 0.7932 - val_loss: 0.4721 - val_acc: 0.7403\n",
      "Epoch 353/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4344 - acc: 0.7866 - val_loss: 0.4786 - val_acc: 0.7403\n",
      "Epoch 354/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4330 - acc: 0.7948 - val_loss: 0.4736 - val_acc: 0.7403\n",
      "Epoch 355/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4338 - acc: 0.7997 - val_loss: 0.4751 - val_acc: 0.7273\n",
      "Epoch 356/400\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.4348 - acc: 0.7899 - val_loss: 0.4821 - val_acc: 0.7468\n",
      "Epoch 357/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4349 - acc: 0.7948 - val_loss: 0.4737 - val_acc: 0.7468\n",
      "Epoch 358/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4345 - acc: 0.7964 - val_loss: 0.4757 - val_acc: 0.7532\n",
      "Epoch 359/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4345 - acc: 0.7915 - val_loss: 0.4762 - val_acc: 0.7273\n",
      "Epoch 360/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4345 - acc: 0.7915 - val_loss: 0.4853 - val_acc: 0.7468\n",
      "Epoch 361/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 44us/step - loss: 0.4337 - acc: 0.7932 - val_loss: 0.4774 - val_acc: 0.7273\n",
      "Epoch 362/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4345 - acc: 0.7964 - val_loss: 0.4754 - val_acc: 0.7338\n",
      "Epoch 363/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4351 - acc: 0.7980 - val_loss: 0.4740 - val_acc: 0.7468\n",
      "Epoch 364/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4352 - acc: 0.7948 - val_loss: 0.4758 - val_acc: 0.7403\n",
      "Epoch 365/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4331 - acc: 0.7834 - val_loss: 0.4878 - val_acc: 0.7532\n",
      "Epoch 366/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4342 - acc: 0.7818 - val_loss: 0.4798 - val_acc: 0.7403\n",
      "Epoch 367/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4346 - acc: 0.7801 - val_loss: 0.4792 - val_acc: 0.7338\n",
      "Epoch 368/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4335 - acc: 0.7899 - val_loss: 0.4807 - val_acc: 0.7468\n",
      "Epoch 369/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4310 - acc: 0.8013 - val_loss: 0.4756 - val_acc: 0.7338\n",
      "Epoch 370/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4354 - acc: 0.7850 - val_loss: 0.4774 - val_acc: 0.7338\n",
      "Epoch 371/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4348 - acc: 0.7899 - val_loss: 0.4775 - val_acc: 0.7403\n",
      "Epoch 372/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4336 - acc: 0.7948 - val_loss: 0.4788 - val_acc: 0.7597\n",
      "Epoch 373/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4338 - acc: 0.7997 - val_loss: 0.4808 - val_acc: 0.7338\n",
      "Epoch 374/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4330 - acc: 0.7948 - val_loss: 0.4898 - val_acc: 0.7532\n",
      "Epoch 375/400\n",
      "614/614 [==============================] - 0s 26us/step - loss: 0.4340 - acc: 0.7915 - val_loss: 0.4747 - val_acc: 0.7468\n",
      "Epoch 376/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4343 - acc: 0.7899 - val_loss: 0.4756 - val_acc: 0.7338\n",
      "Epoch 377/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4336 - acc: 0.7915 - val_loss: 0.4749 - val_acc: 0.7273\n",
      "Epoch 378/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4307 - acc: 0.7948 - val_loss: 0.4855 - val_acc: 0.7468\n",
      "Epoch 379/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4337 - acc: 0.7948 - val_loss: 0.4720 - val_acc: 0.7338\n",
      "Epoch 380/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4331 - acc: 0.7964 - val_loss: 0.4728 - val_acc: 0.7468\n",
      "Epoch 381/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4340 - acc: 0.7866 - val_loss: 0.4739 - val_acc: 0.7273\n",
      "Epoch 382/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4343 - acc: 0.7964 - val_loss: 0.4765 - val_acc: 0.7273\n",
      "Epoch 383/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4298 - acc: 0.7948 - val_loss: 0.4841 - val_acc: 0.7468\n",
      "Epoch 384/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4349 - acc: 0.7948 - val_loss: 0.4710 - val_acc: 0.7597\n",
      "Epoch 385/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.4788 - val_acc: 0.7273\n",
      "Epoch 386/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.4322 - acc: 0.8029 - val_loss: 0.4854 - val_acc: 0.7468\n",
      "Epoch 387/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4345 - acc: 0.7932 - val_loss: 0.4758 - val_acc: 0.7273\n",
      "Epoch 388/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4340 - acc: 0.7932 - val_loss: 0.4729 - val_acc: 0.7338\n",
      "Epoch 389/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4348 - acc: 0.7850 - val_loss: 0.4883 - val_acc: 0.7468\n",
      "Epoch 390/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4328 - acc: 0.7915 - val_loss: 0.4750 - val_acc: 0.7403\n",
      "Epoch 391/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.4328 - acc: 0.7850 - val_loss: 0.4816 - val_acc: 0.7403\n",
      "Epoch 392/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4329 - acc: 0.7964 - val_loss: 0.4745 - val_acc: 0.7273\n",
      "Epoch 393/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4341 - acc: 0.7915 - val_loss: 0.4732 - val_acc: 0.7403\n",
      "Epoch 394/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4312 - acc: 0.7883 - val_loss: 0.4784 - val_acc: 0.7273\n",
      "Epoch 395/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4323 - acc: 0.7850 - val_loss: 0.4768 - val_acc: 0.7273\n",
      "Epoch 396/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4348 - acc: 0.7915 - val_loss: 0.4764 - val_acc: 0.7273\n",
      "Epoch 397/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4332 - acc: 0.7964 - val_loss: 0.4820 - val_acc: 0.7403\n",
      "Epoch 398/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.4328 - acc: 0.7997 - val_loss: 0.4826 - val_acc: 0.7403\n",
      "Epoch 399/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4333 - acc: 0.7883 - val_loss: 0.4847 - val_acc: 0.7468\n",
      "Epoch 400/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4343 - acc: 0.7899 - val_loss: 0.4782 - val_acc: 0.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b6559683c8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation='relu', input_dim=8))\n",
    "\n",
    "model.add(Dense(10,activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(df_mean[input_columns], df[8], batch_size=32, epochs=400, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/768 with 1 missing, elapsed time: 0.137\n",
      "Imputing row 101/768 with 2 missing, elapsed time: 0.141\n",
      "Imputing row 201/768 with 2 missing, elapsed time: 0.143\n",
      "Imputing row 301/768 with 3 missing, elapsed time: 0.145\n",
      "Imputing row 401/768 with 2 missing, elapsed time: 0.147\n",
      "Imputing row 501/768 with 1 missing, elapsed time: 0.150\n",
      "Imputing row 601/768 with 1 missing, elapsed time: 0.153\n",
      "Imputing row 701/768 with 0 missing, elapsed time: 0.155\n",
      "Train on 614 samples, validate on 154 samples\n",
      "Epoch 1/400\n",
      "614/614 [==============================] - 0s 452us/step - loss: 0.7150 - acc: 0.3534 - val_loss: 0.6935 - val_acc: 0.5130\n",
      "Epoch 2/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6839 - acc: 0.6384 - val_loss: 0.6720 - val_acc: 0.6558\n",
      "Epoch 3/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6678 - acc: 0.6661 - val_loss: 0.6584 - val_acc: 0.6429\n",
      "Epoch 4/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.6553 - acc: 0.6612 - val_loss: 0.6488 - val_acc: 0.6429\n",
      "Epoch 5/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.6476 - acc: 0.6547 - val_loss: 0.6434 - val_acc: 0.6429\n",
      "Epoch 6/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.6421 - acc: 0.6596 - val_loss: 0.6377 - val_acc: 0.6429\n",
      "Epoch 7/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.6370 - acc: 0.6612 - val_loss: 0.6328 - val_acc: 0.6429\n",
      "Epoch 8/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.6323 - acc: 0.6580 - val_loss: 0.6283 - val_acc: 0.6429\n",
      "Epoch 9/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.6268 - acc: 0.6629 - val_loss: 0.6233 - val_acc: 0.6494\n",
      "Epoch 10/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6221 - acc: 0.6645 - val_loss: 0.6180 - val_acc: 0.6494\n",
      "Epoch 11/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.6170 - acc: 0.6629 - val_loss: 0.6133 - val_acc: 0.6623\n",
      "Epoch 12/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.6118 - acc: 0.6596 - val_loss: 0.6084 - val_acc: 0.6753\n",
      "Epoch 13/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.6075 - acc: 0.6694 - val_loss: 0.6045 - val_acc: 0.6753\n",
      "Epoch 14/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.6031 - acc: 0.6661 - val_loss: 0.5992 - val_acc: 0.6948\n",
      "Epoch 15/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.5988 - acc: 0.6743 - val_loss: 0.5949 - val_acc: 0.6948\n",
      "Epoch 16/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.5943 - acc: 0.6808 - val_loss: 0.5901 - val_acc: 0.7078\n",
      "Epoch 17/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.5891 - acc: 0.6857 - val_loss: 0.5846 - val_acc: 0.7208\n",
      "Epoch 18/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.5834 - acc: 0.6971 - val_loss: 0.5803 - val_acc: 0.6948\n",
      "Epoch 19/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.5793 - acc: 0.6873 - val_loss: 0.5757 - val_acc: 0.7078\n",
      "Epoch 20/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.5742 - acc: 0.6938 - val_loss: 0.5712 - val_acc: 0.7013\n",
      "Epoch 21/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.5694 - acc: 0.6938 - val_loss: 0.5650 - val_acc: 0.7078\n",
      "Epoch 22/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.5646 - acc: 0.7134 - val_loss: 0.5601 - val_acc: 0.7078\n",
      "Epoch 23/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.5605 - acc: 0.7166 - val_loss: 0.5560 - val_acc: 0.7078\n",
      "Epoch 24/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.5568 - acc: 0.7150 - val_loss: 0.5522 - val_acc: 0.7143\n",
      "Epoch 25/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.5523 - acc: 0.7215 - val_loss: 0.5483 - val_acc: 0.7143\n",
      "Epoch 26/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.5477 - acc: 0.7264 - val_loss: 0.5450 - val_acc: 0.7273\n",
      "Epoch 27/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.5436 - acc: 0.7150 - val_loss: 0.5403 - val_acc: 0.7273\n",
      "Epoch 28/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.5399 - acc: 0.7443 - val_loss: 0.5404 - val_acc: 0.7273\n",
      "Epoch 29/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.5392 - acc: 0.7215 - val_loss: 0.5339 - val_acc: 0.7208\n",
      "Epoch 30/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.5354 - acc: 0.7443 - val_loss: 0.5309 - val_acc: 0.7208\n",
      "Epoch 31/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.5316 - acc: 0.7378 - val_loss: 0.5260 - val_acc: 0.7338\n",
      "Epoch 32/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.5280 - acc: 0.7541 - val_loss: 0.5233 - val_acc: 0.7273\n",
      "Epoch 33/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.5247 - acc: 0.7573 - val_loss: 0.5220 - val_acc: 0.7273\n",
      "Epoch 34/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.5226 - acc: 0.7476 - val_loss: 0.5213 - val_acc: 0.7208\n",
      "Epoch 35/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.5198 - acc: 0.7476 - val_loss: 0.5147 - val_acc: 0.7338\n",
      "Epoch 36/400\n",
      "614/614 [==============================] - 0s 28us/step - loss: 0.5168 - acc: 0.7573 - val_loss: 0.5114 - val_acc: 0.7468\n",
      "Epoch 37/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.5153 - acc: 0.7590 - val_loss: 0.5094 - val_acc: 0.7338\n",
      "Epoch 38/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.5116 - acc: 0.7638 - val_loss: 0.5098 - val_acc: 0.7338\n",
      "Epoch 39/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.5088 - acc: 0.7541 - val_loss: 0.5048 - val_acc: 0.7532\n",
      "Epoch 40/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.5083 - acc: 0.7622 - val_loss: 0.5054 - val_acc: 0.7338\n",
      "Epoch 41/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.5064 - acc: 0.7687 - val_loss: 0.5019 - val_acc: 0.7727\n",
      "Epoch 42/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.5047 - acc: 0.7541 - val_loss: 0.4991 - val_acc: 0.7468\n",
      "Epoch 43/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.5035 - acc: 0.7638 - val_loss: 0.4981 - val_acc: 0.7532\n",
      "Epoch 44/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.5013 - acc: 0.7622 - val_loss: 0.4970 - val_acc: 0.7532\n",
      "Epoch 45/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.5001 - acc: 0.7655 - val_loss: 0.4943 - val_acc: 0.7597\n",
      "Epoch 46/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4982 - acc: 0.7573 - val_loss: 0.4936 - val_acc: 0.7597\n",
      "Epoch 47/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4969 - acc: 0.7606 - val_loss: 0.4917 - val_acc: 0.7597\n",
      "Epoch 48/400\n",
      "614/614 [==============================] - ETA: 0s - loss: 0.6083 - acc: 0.687 - 0s 31us/step - loss: 0.4946 - acc: 0.7557 - val_loss: 0.4928 - val_acc: 0.7468\n",
      "Epoch 49/400\n",
      "614/614 [==============================] - 0s 29us/step - loss: 0.4938 - acc: 0.7638 - val_loss: 0.4889 - val_acc: 0.7532\n",
      "Epoch 50/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4914 - acc: 0.7655 - val_loss: 0.4857 - val_acc: 0.7662\n",
      "Epoch 51/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4908 - acc: 0.7557 - val_loss: 0.4851 - val_acc: 0.7597\n",
      "Epoch 52/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4899 - acc: 0.7606 - val_loss: 0.4837 - val_acc: 0.7662\n",
      "Epoch 53/400\n",
      "614/614 [==============================] - 0s 31us/step - loss: 0.4881 - acc: 0.7655 - val_loss: 0.4824 - val_acc: 0.7662\n",
      "Epoch 54/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4873 - acc: 0.7557 - val_loss: 0.4856 - val_acc: 0.7597\n",
      "Epoch 55/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4873 - acc: 0.7638 - val_loss: 0.4801 - val_acc: 0.7597\n",
      "Epoch 56/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4855 - acc: 0.7590 - val_loss: 0.4798 - val_acc: 0.7727\n",
      "Epoch 57/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4849 - acc: 0.7557 - val_loss: 0.4783 - val_acc: 0.7727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4832 - acc: 0.7638 - val_loss: 0.4796 - val_acc: 0.7857\n",
      "Epoch 59/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4832 - acc: 0.7655 - val_loss: 0.4774 - val_acc: 0.7727\n",
      "Epoch 60/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4817 - acc: 0.7687 - val_loss: 0.4761 - val_acc: 0.7727\n",
      "Epoch 61/400\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4798 - acc: 0.7720 - val_loss: 0.4826 - val_acc: 0.7597\n",
      "Epoch 62/400\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.4806 - acc: 0.7671 - val_loss: 0.4743 - val_acc: 0.7792\n",
      "Epoch 63/400\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4782 - acc: 0.7638 - val_loss: 0.4802 - val_acc: 0.7597\n",
      "Epoch 64/400\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4784 - acc: 0.7736 - val_loss: 0.4727 - val_acc: 0.7727\n",
      "Epoch 65/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4773 - acc: 0.7655 - val_loss: 0.4721 - val_acc: 0.7857\n",
      "Epoch 66/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4764 - acc: 0.7720 - val_loss: 0.4735 - val_acc: 0.7597\n",
      "Epoch 67/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4762 - acc: 0.7655 - val_loss: 0.4703 - val_acc: 0.7857\n",
      "Epoch 68/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4747 - acc: 0.7671 - val_loss: 0.4697 - val_acc: 0.7792\n",
      "Epoch 69/400\n",
      "614/614 [==============================] - 0s 88us/step - loss: 0.4742 - acc: 0.7752 - val_loss: 0.4682 - val_acc: 0.7857\n",
      "Epoch 70/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4739 - acc: 0.7671 - val_loss: 0.4708 - val_acc: 0.7532\n",
      "Epoch 71/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4727 - acc: 0.7687 - val_loss: 0.4663 - val_acc: 0.7922\n",
      "Epoch 72/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4724 - acc: 0.7736 - val_loss: 0.4663 - val_acc: 0.7922\n",
      "Epoch 73/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4705 - acc: 0.7769 - val_loss: 0.4713 - val_acc: 0.7532\n",
      "Epoch 74/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4721 - acc: 0.7606 - val_loss: 0.4702 - val_acc: 0.7532\n",
      "Epoch 75/400\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.4707 - acc: 0.7687 - val_loss: 0.4644 - val_acc: 0.7792\n",
      "Epoch 76/400\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4691 - acc: 0.7655 - val_loss: 0.4633 - val_acc: 0.7922\n",
      "Epoch 77/400\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4692 - acc: 0.7720 - val_loss: 0.4645 - val_acc: 0.7857\n",
      "Epoch 78/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4689 - acc: 0.7752 - val_loss: 0.4631 - val_acc: 0.7857\n",
      "Epoch 79/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4688 - acc: 0.7801 - val_loss: 0.4624 - val_acc: 0.7857\n",
      "Epoch 80/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4676 - acc: 0.7785 - val_loss: 0.4656 - val_acc: 0.7922\n",
      "Epoch 81/400\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.4691 - acc: 0.7720 - val_loss: 0.4606 - val_acc: 0.7857\n",
      "Epoch 82/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4664 - acc: 0.7801 - val_loss: 0.4622 - val_acc: 0.7922\n",
      "Epoch 83/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4663 - acc: 0.7752 - val_loss: 0.4610 - val_acc: 0.7857\n",
      "Epoch 84/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4651 - acc: 0.7736 - val_loss: 0.4591 - val_acc: 0.7922\n",
      "Epoch 85/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4657 - acc: 0.7720 - val_loss: 0.4588 - val_acc: 0.7857\n",
      "Epoch 86/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4651 - acc: 0.7785 - val_loss: 0.4601 - val_acc: 0.7987\n",
      "Epoch 87/400\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4647 - acc: 0.7704 - val_loss: 0.4582 - val_acc: 0.7857\n",
      "Epoch 88/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4628 - acc: 0.7801 - val_loss: 0.4596 - val_acc: 0.7857\n",
      "Epoch 89/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4636 - acc: 0.7769 - val_loss: 0.4573 - val_acc: 0.7857\n",
      "Epoch 90/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4635 - acc: 0.7801 - val_loss: 0.4569 - val_acc: 0.7922\n",
      "Epoch 91/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4608 - acc: 0.7801 - val_loss: 0.4667 - val_acc: 0.7597\n",
      "Epoch 92/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4624 - acc: 0.7752 - val_loss: 0.4552 - val_acc: 0.7727\n",
      "Epoch 93/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4614 - acc: 0.7736 - val_loss: 0.4567 - val_acc: 0.7987\n",
      "Epoch 94/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4608 - acc: 0.7834 - val_loss: 0.4552 - val_acc: 0.7922\n",
      "Epoch 95/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4604 - acc: 0.7785 - val_loss: 0.4532 - val_acc: 0.7857\n",
      "Epoch 96/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4599 - acc: 0.7769 - val_loss: 0.4580 - val_acc: 0.7727\n",
      "Epoch 97/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4596 - acc: 0.7801 - val_loss: 0.4536 - val_acc: 0.7922\n",
      "Epoch 98/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4580 - acc: 0.7818 - val_loss: 0.4611 - val_acc: 0.7857\n",
      "Epoch 99/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4597 - acc: 0.7866 - val_loss: 0.4544 - val_acc: 0.7922\n",
      "Epoch 100/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4571 - acc: 0.7834 - val_loss: 0.4636 - val_acc: 0.7597\n",
      "Epoch 101/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4584 - acc: 0.7834 - val_loss: 0.4526 - val_acc: 0.8052\n",
      "Epoch 102/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4574 - acc: 0.7785 - val_loss: 0.4517 - val_acc: 0.7987\n",
      "Epoch 103/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4561 - acc: 0.7818 - val_loss: 0.4525 - val_acc: 0.7987\n",
      "Epoch 104/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4562 - acc: 0.7834 - val_loss: 0.4503 - val_acc: 0.7857\n",
      "Epoch 105/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4559 - acc: 0.7785 - val_loss: 0.4503 - val_acc: 0.8052\n",
      "Epoch 106/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4558 - acc: 0.7752 - val_loss: 0.4508 - val_acc: 0.7922\n",
      "Epoch 107/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4550 - acc: 0.7850 - val_loss: 0.4493 - val_acc: 0.7987\n",
      "Epoch 108/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4545 - acc: 0.7801 - val_loss: 0.4499 - val_acc: 0.8052\n",
      "Epoch 109/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4536 - acc: 0.7818 - val_loss: 0.4494 - val_acc: 0.7857\n",
      "Epoch 110/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4536 - acc: 0.7818 - val_loss: 0.4484 - val_acc: 0.7987\n",
      "Epoch 111/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4536 - acc: 0.7801 - val_loss: 0.4507 - val_acc: 0.7922\n",
      "Epoch 112/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4524 - acc: 0.7866 - val_loss: 0.4482 - val_acc: 0.7922\n",
      "Epoch 113/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4517 - acc: 0.7801 - val_loss: 0.4499 - val_acc: 0.8052\n",
      "Epoch 114/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4516 - acc: 0.7850 - val_loss: 0.4481 - val_acc: 0.8052\n",
      "Epoch 115/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4513 - acc: 0.7818 - val_loss: 0.4499 - val_acc: 0.8117\n",
      "Epoch 116/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4496 - acc: 0.7899 - val_loss: 0.4546 - val_acc: 0.7727\n",
      "Epoch 117/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4513 - acc: 0.7834 - val_loss: 0.4542 - val_acc: 0.7727\n",
      "Epoch 118/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 45us/step - loss: 0.4517 - acc: 0.7801 - val_loss: 0.4488 - val_acc: 0.7922\n",
      "Epoch 119/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4506 - acc: 0.7883 - val_loss: 0.4476 - val_acc: 0.7922\n",
      "Epoch 120/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4476 - acc: 0.7818 - val_loss: 0.4451 - val_acc: 0.7857\n",
      "Epoch 121/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4498 - acc: 0.7866 - val_loss: 0.4440 - val_acc: 0.7987\n",
      "Epoch 122/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4485 - acc: 0.7899 - val_loss: 0.4488 - val_acc: 0.7792\n",
      "Epoch 123/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4472 - acc: 0.7785 - val_loss: 0.4446 - val_acc: 0.7922\n",
      "Epoch 124/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4478 - acc: 0.7834 - val_loss: 0.4456 - val_acc: 0.7922\n",
      "Epoch 125/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4472 - acc: 0.7785 - val_loss: 0.4432 - val_acc: 0.7857\n",
      "Epoch 126/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4474 - acc: 0.7850 - val_loss: 0.4513 - val_acc: 0.7727\n",
      "Epoch 127/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4471 - acc: 0.7850 - val_loss: 0.4451 - val_acc: 0.8117\n",
      "Epoch 128/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4471 - acc: 0.7801 - val_loss: 0.4446 - val_acc: 0.8117\n",
      "Epoch 129/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4459 - acc: 0.7883 - val_loss: 0.4476 - val_acc: 0.7792\n",
      "Epoch 130/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4461 - acc: 0.7769 - val_loss: 0.4437 - val_acc: 0.7922\n",
      "Epoch 131/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4454 - acc: 0.7850 - val_loss: 0.4425 - val_acc: 0.7922\n",
      "Epoch 132/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4443 - acc: 0.7899 - val_loss: 0.4431 - val_acc: 0.8117\n",
      "Epoch 133/400\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.4447 - acc: 0.7932 - val_loss: 0.4415 - val_acc: 0.7857\n",
      "Epoch 134/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4443 - acc: 0.7850 - val_loss: 0.4417 - val_acc: 0.8117\n",
      "Epoch 135/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4421 - acc: 0.7834 - val_loss: 0.4422 - val_acc: 0.7857\n",
      "Epoch 136/400\n",
      "614/614 [==============================] - 0s 71us/step - loss: 0.4435 - acc: 0.7899 - val_loss: 0.4423 - val_acc: 0.7857\n",
      "Epoch 137/400\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.4427 - acc: 0.7785 - val_loss: 0.4425 - val_acc: 0.8117\n",
      "Epoch 138/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4425 - acc: 0.7834 - val_loss: 0.4395 - val_acc: 0.7857\n",
      "Epoch 139/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4430 - acc: 0.7850 - val_loss: 0.4387 - val_acc: 0.7922\n",
      "Epoch 140/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4417 - acc: 0.7899 - val_loss: 0.4387 - val_acc: 0.8052\n",
      "Epoch 141/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4414 - acc: 0.7866 - val_loss: 0.4387 - val_acc: 0.7857\n",
      "Epoch 142/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4410 - acc: 0.7915 - val_loss: 0.4370 - val_acc: 0.7857\n",
      "Epoch 143/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4411 - acc: 0.7850 - val_loss: 0.4399 - val_acc: 0.7857\n",
      "Epoch 144/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4408 - acc: 0.7883 - val_loss: 0.4388 - val_acc: 0.7857\n",
      "Epoch 145/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4418 - acc: 0.7915 - val_loss: 0.4369 - val_acc: 0.7857\n",
      "Epoch 146/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4399 - acc: 0.7932 - val_loss: 0.4383 - val_acc: 0.8117\n",
      "Epoch 147/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4395 - acc: 0.7915 - val_loss: 0.4366 - val_acc: 0.7857\n",
      "Epoch 148/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4398 - acc: 0.7899 - val_loss: 0.4363 - val_acc: 0.7922\n",
      "Epoch 149/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4387 - acc: 0.7932 - val_loss: 0.4420 - val_acc: 0.8182\n",
      "Epoch 150/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4388 - acc: 0.7932 - val_loss: 0.4360 - val_acc: 0.7922\n",
      "Epoch 151/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4386 - acc: 0.7915 - val_loss: 0.4406 - val_acc: 0.8247\n",
      "Epoch 152/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4395 - acc: 0.7883 - val_loss: 0.4347 - val_acc: 0.7922\n",
      "Epoch 153/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4380 - acc: 0.7883 - val_loss: 0.4345 - val_acc: 0.7857\n",
      "Epoch 154/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4362 - acc: 0.7818 - val_loss: 0.4375 - val_acc: 0.7857\n",
      "Epoch 155/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4383 - acc: 0.7915 - val_loss: 0.4423 - val_acc: 0.7857\n",
      "Epoch 156/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4371 - acc: 0.7834 - val_loss: 0.4344 - val_acc: 0.8117\n",
      "Epoch 157/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4354 - acc: 0.7866 - val_loss: 0.4389 - val_acc: 0.7792\n",
      "Epoch 158/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4365 - acc: 0.7915 - val_loss: 0.4346 - val_acc: 0.7922\n",
      "Epoch 159/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4365 - acc: 0.7915 - val_loss: 0.4335 - val_acc: 0.7922\n",
      "Epoch 160/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4365 - acc: 0.7948 - val_loss: 0.4353 - val_acc: 0.8052\n",
      "Epoch 161/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4354 - acc: 0.7866 - val_loss: 0.4356 - val_acc: 0.8117\n",
      "Epoch 162/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4349 - acc: 0.7932 - val_loss: 0.4332 - val_acc: 0.7857\n",
      "Epoch 163/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4339 - acc: 0.7932 - val_loss: 0.4382 - val_acc: 0.7792\n",
      "Epoch 164/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4358 - acc: 0.7850 - val_loss: 0.4357 - val_acc: 0.8117\n",
      "Epoch 165/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4339 - acc: 0.7932 - val_loss: 0.4335 - val_acc: 0.7922\n",
      "Epoch 166/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4350 - acc: 0.7915 - val_loss: 0.4331 - val_acc: 0.7922\n",
      "Epoch 167/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4330 - acc: 0.7883 - val_loss: 0.4343 - val_acc: 0.8182\n",
      "Epoch 168/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4331 - acc: 0.7997 - val_loss: 0.4342 - val_acc: 0.7857\n",
      "Epoch 169/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4324 - acc: 0.7932 - val_loss: 0.4330 - val_acc: 0.7792\n",
      "Epoch 170/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4333 - acc: 0.7932 - val_loss: 0.4323 - val_acc: 0.7857\n",
      "Epoch 171/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4329 - acc: 0.7948 - val_loss: 0.4371 - val_acc: 0.7792\n",
      "Epoch 172/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4335 - acc: 0.7948 - val_loss: 0.4346 - val_acc: 0.7857\n",
      "Epoch 173/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4320 - acc: 0.7932 - val_loss: 0.4327 - val_acc: 0.7792\n",
      "Epoch 174/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4313 - acc: 0.7915 - val_loss: 0.4342 - val_acc: 0.7857\n",
      "Epoch 175/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4330 - acc: 0.7899 - val_loss: 0.4311 - val_acc: 0.7922\n",
      "Epoch 176/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4307 - acc: 0.7899 - val_loss: 0.4311 - val_acc: 0.7922\n",
      "Epoch 177/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4311 - acc: 0.7948 - val_loss: 0.4340 - val_acc: 0.8052\n",
      "Epoch 178/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 55us/step - loss: 0.4309 - acc: 0.7883 - val_loss: 0.4320 - val_acc: 0.7857\n",
      "Epoch 179/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4296 - acc: 0.7932 - val_loss: 0.4323 - val_acc: 0.7922\n",
      "Epoch 180/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4301 - acc: 0.7948 - val_loss: 0.4314 - val_acc: 0.7922\n",
      "Epoch 181/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4302 - acc: 0.7866 - val_loss: 0.4306 - val_acc: 0.7922\n",
      "Epoch 182/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4294 - acc: 0.7915 - val_loss: 0.4317 - val_acc: 0.7922\n",
      "Epoch 183/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4284 - acc: 0.7899 - val_loss: 0.4334 - val_acc: 0.8247\n",
      "Epoch 184/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4306 - acc: 0.7866 - val_loss: 0.4294 - val_acc: 0.7922\n",
      "Epoch 185/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4284 - acc: 0.7997 - val_loss: 0.4316 - val_acc: 0.8052\n",
      "Epoch 186/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4298 - acc: 0.7915 - val_loss: 0.4308 - val_acc: 0.8117\n",
      "Epoch 187/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4280 - acc: 0.7980 - val_loss: 0.4307 - val_acc: 0.7792\n",
      "Epoch 188/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4285 - acc: 0.7915 - val_loss: 0.4342 - val_acc: 0.8247\n",
      "Epoch 189/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4289 - acc: 0.7850 - val_loss: 0.4288 - val_acc: 0.7922\n",
      "Epoch 190/400\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.4282 - acc: 0.7948 - val_loss: 0.4293 - val_acc: 0.7987\n",
      "Epoch 191/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4279 - acc: 0.7915 - val_loss: 0.4285 - val_acc: 0.7987\n",
      "Epoch 192/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4238 - acc: 0.8013 - val_loss: 0.4308 - val_acc: 0.7857\n",
      "Epoch 193/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4275 - acc: 0.7866 - val_loss: 0.4292 - val_acc: 0.7792\n",
      "Epoch 194/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4270 - acc: 0.7866 - val_loss: 0.4279 - val_acc: 0.7922\n",
      "Epoch 195/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4262 - acc: 0.7932 - val_loss: 0.4270 - val_acc: 0.7922\n",
      "Epoch 196/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4260 - acc: 0.7964 - val_loss: 0.4284 - val_acc: 0.7857\n",
      "Epoch 197/400\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.4244 - acc: 0.7997 - val_loss: 0.4289 - val_acc: 0.8052\n",
      "Epoch 198/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4259 - acc: 0.7850 - val_loss: 0.4282 - val_acc: 0.7922\n",
      "Epoch 199/400\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.4247 - acc: 0.7964 - val_loss: 0.4314 - val_acc: 0.8312\n",
      "Epoch 200/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4255 - acc: 0.7932 - val_loss: 0.4330 - val_acc: 0.8312\n",
      "Epoch 201/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4264 - acc: 0.7899 - val_loss: 0.4260 - val_acc: 0.7922\n",
      "Epoch 202/400\n",
      "614/614 [==============================] - 0s 67us/step - loss: 0.4245 - acc: 0.7948 - val_loss: 0.4265 - val_acc: 0.7922\n",
      "Epoch 203/400\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.4251 - acc: 0.7948 - val_loss: 0.4286 - val_acc: 0.8117\n",
      "Epoch 204/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4238 - acc: 0.7964 - val_loss: 0.4315 - val_acc: 0.8182\n",
      "Epoch 205/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4249 - acc: 0.7915 - val_loss: 0.4257 - val_acc: 0.7922\n",
      "Epoch 206/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4251 - acc: 0.7932 - val_loss: 0.4261 - val_acc: 0.7922\n",
      "Epoch 207/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4235 - acc: 0.7899 - val_loss: 0.4277 - val_acc: 0.7922\n",
      "Epoch 208/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4249 - acc: 0.7980 - val_loss: 0.4254 - val_acc: 0.7987\n",
      "Epoch 209/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4220 - acc: 0.7948 - val_loss: 0.4273 - val_acc: 0.7922\n",
      "Epoch 210/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4224 - acc: 0.7997 - val_loss: 0.4277 - val_acc: 0.8182\n",
      "Epoch 211/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4236 - acc: 0.7915 - val_loss: 0.4243 - val_acc: 0.7922\n",
      "Epoch 212/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4213 - acc: 0.7980 - val_loss: 0.4242 - val_acc: 0.7922\n",
      "Epoch 213/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4217 - acc: 0.7948 - val_loss: 0.4242 - val_acc: 0.7987\n",
      "Epoch 214/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4211 - acc: 0.7948 - val_loss: 0.4262 - val_acc: 0.7922\n",
      "Epoch 215/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4221 - acc: 0.7997 - val_loss: 0.4261 - val_acc: 0.8052\n",
      "Epoch 216/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4209 - acc: 0.7964 - val_loss: 0.4266 - val_acc: 0.7922\n",
      "Epoch 217/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4210 - acc: 0.7883 - val_loss: 0.4265 - val_acc: 0.7922\n",
      "Epoch 218/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4214 - acc: 0.7948 - val_loss: 0.4250 - val_acc: 0.7922\n",
      "Epoch 219/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4210 - acc: 0.7915 - val_loss: 0.4247 - val_acc: 0.7922\n",
      "Epoch 220/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4199 - acc: 0.7915 - val_loss: 0.4243 - val_acc: 0.7922\n",
      "Epoch 221/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4197 - acc: 0.7980 - val_loss: 0.4260 - val_acc: 0.8182\n",
      "Epoch 222/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4207 - acc: 0.7915 - val_loss: 0.4238 - val_acc: 0.7922\n",
      "Epoch 223/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4195 - acc: 0.7964 - val_loss: 0.4261 - val_acc: 0.8182\n",
      "Epoch 224/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4171 - acc: 0.7980 - val_loss: 0.4254 - val_acc: 0.7922\n",
      "Epoch 225/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4190 - acc: 0.7899 - val_loss: 0.4259 - val_acc: 0.8052\n",
      "Epoch 226/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4201 - acc: 0.7948 - val_loss: 0.4256 - val_acc: 0.7922\n",
      "Epoch 227/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4185 - acc: 0.7980 - val_loss: 0.4258 - val_acc: 0.7922\n",
      "Epoch 228/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4185 - acc: 0.7980 - val_loss: 0.4238 - val_acc: 0.7922\n",
      "Epoch 229/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4174 - acc: 0.7899 - val_loss: 0.4240 - val_acc: 0.7922\n",
      "Epoch 230/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4172 - acc: 0.7948 - val_loss: 0.4239 - val_acc: 0.7922\n",
      "Epoch 231/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4175 - acc: 0.7915 - val_loss: 0.4226 - val_acc: 0.7922\n",
      "Epoch 232/400\n",
      "614/614 [==============================] - 0s 32us/step - loss: 0.4169 - acc: 0.7932 - val_loss: 0.4220 - val_acc: 0.7922\n",
      "Epoch 233/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4165 - acc: 0.8013 - val_loss: 0.4216 - val_acc: 0.7922\n",
      "Epoch 234/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4165 - acc: 0.7948 - val_loss: 0.4219 - val_acc: 0.7922\n",
      "Epoch 235/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4177 - acc: 0.7948 - val_loss: 0.4205 - val_acc: 0.7922\n",
      "Epoch 236/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4165 - acc: 0.7948 - val_loss: 0.4226 - val_acc: 0.7922\n",
      "Epoch 237/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4175 - acc: 0.7915 - val_loss: 0.4228 - val_acc: 0.7922\n",
      "Epoch 238/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 47us/step - loss: 0.4164 - acc: 0.7899 - val_loss: 0.4211 - val_acc: 0.8117\n",
      "Epoch 239/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4167 - acc: 0.7932 - val_loss: 0.4230 - val_acc: 0.8247\n",
      "Epoch 240/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4156 - acc: 0.7948 - val_loss: 0.4220 - val_acc: 0.7922\n",
      "Epoch 241/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4162 - acc: 0.7915 - val_loss: 0.4258 - val_acc: 0.7857\n",
      "Epoch 242/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4158 - acc: 0.7948 - val_loss: 0.4206 - val_acc: 0.7987\n",
      "Epoch 243/400\n",
      "614/614 [==============================] - 0s 34us/step - loss: 0.4146 - acc: 0.7915 - val_loss: 0.4220 - val_acc: 0.8182\n",
      "Epoch 244/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4132 - acc: 0.7948 - val_loss: 0.4191 - val_acc: 0.7922\n",
      "Epoch 245/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4155 - acc: 0.7932 - val_loss: 0.4196 - val_acc: 0.7987\n",
      "Epoch 246/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4140 - acc: 0.7883 - val_loss: 0.4199 - val_acc: 0.7987\n",
      "Epoch 247/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4140 - acc: 0.7964 - val_loss: 0.4213 - val_acc: 0.7922\n",
      "Epoch 248/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4137 - acc: 0.7932 - val_loss: 0.4216 - val_acc: 0.7922\n",
      "Epoch 249/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4134 - acc: 0.8029 - val_loss: 0.4208 - val_acc: 0.7922\n",
      "Epoch 250/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4129 - acc: 0.7980 - val_loss: 0.4199 - val_acc: 0.7922\n",
      "Epoch 251/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.4133 - acc: 0.7915 - val_loss: 0.4220 - val_acc: 0.8182\n",
      "Epoch 252/400\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4131 - acc: 0.7932 - val_loss: 0.4208 - val_acc: 0.7987\n",
      "Epoch 253/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4129 - acc: 0.7915 - val_loss: 0.4217 - val_acc: 0.7922\n",
      "Epoch 254/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4115 - acc: 0.7948 - val_loss: 0.4264 - val_acc: 0.8247\n",
      "Epoch 255/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.4133 - acc: 0.7980 - val_loss: 0.4194 - val_acc: 0.8117\n",
      "Epoch 256/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4115 - acc: 0.7948 - val_loss: 0.4202 - val_acc: 0.7922\n",
      "Epoch 257/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4114 - acc: 0.7948 - val_loss: 0.4198 - val_acc: 0.8182\n",
      "Epoch 258/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4121 - acc: 0.7948 - val_loss: 0.4184 - val_acc: 0.8117\n",
      "Epoch 259/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4117 - acc: 0.7883 - val_loss: 0.4178 - val_acc: 0.8117\n",
      "Epoch 260/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4116 - acc: 0.7980 - val_loss: 0.4192 - val_acc: 0.8117\n",
      "Epoch 261/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4120 - acc: 0.7964 - val_loss: 0.4203 - val_acc: 0.7922\n",
      "Epoch 262/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4110 - acc: 0.7932 - val_loss: 0.4190 - val_acc: 0.8117\n",
      "Epoch 263/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4098 - acc: 0.8062 - val_loss: 0.4189 - val_acc: 0.7922\n",
      "Epoch 264/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.4104 - acc: 0.7948 - val_loss: 0.4180 - val_acc: 0.8117\n",
      "Epoch 265/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4093 - acc: 0.7899 - val_loss: 0.4193 - val_acc: 0.8182\n",
      "Epoch 266/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4103 - acc: 0.7932 - val_loss: 0.4193 - val_acc: 0.8182\n",
      "Epoch 267/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4094 - acc: 0.7997 - val_loss: 0.4196 - val_acc: 0.8117\n",
      "Epoch 268/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4085 - acc: 0.8046 - val_loss: 0.4218 - val_acc: 0.8182\n",
      "Epoch 269/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4106 - acc: 0.7899 - val_loss: 0.4187 - val_acc: 0.8117\n",
      "Epoch 270/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4111 - acc: 0.7932 - val_loss: 0.4205 - val_acc: 0.8247\n",
      "Epoch 271/400\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.4092 - acc: 0.7964 - val_loss: 0.4246 - val_acc: 0.7922\n",
      "Epoch 272/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4109 - acc: 0.7915 - val_loss: 0.4172 - val_acc: 0.8117\n",
      "Epoch 273/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4093 - acc: 0.7980 - val_loss: 0.4179 - val_acc: 0.8052\n",
      "Epoch 274/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4099 - acc: 0.7948 - val_loss: 0.4163 - val_acc: 0.8182\n",
      "Epoch 275/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4081 - acc: 0.8013 - val_loss: 0.4195 - val_acc: 0.8247\n",
      "Epoch 276/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4085 - acc: 0.8013 - val_loss: 0.4173 - val_acc: 0.8117\n",
      "Epoch 277/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4083 - acc: 0.7932 - val_loss: 0.4230 - val_acc: 0.8312\n",
      "Epoch 278/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4096 - acc: 0.7899 - val_loss: 0.4266 - val_acc: 0.8377\n",
      "Epoch 279/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4068 - acc: 0.7980 - val_loss: 0.4174 - val_acc: 0.8117\n",
      "Epoch 280/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4077 - acc: 0.7980 - val_loss: 0.4175 - val_acc: 0.8117\n",
      "Epoch 281/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4073 - acc: 0.7964 - val_loss: 0.4174 - val_acc: 0.8117\n",
      "Epoch 282/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4083 - acc: 0.7980 - val_loss: 0.4166 - val_acc: 0.8182\n",
      "Epoch 283/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4064 - acc: 0.7932 - val_loss: 0.4196 - val_acc: 0.8052\n",
      "Epoch 284/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4069 - acc: 0.7948 - val_loss: 0.4174 - val_acc: 0.8182\n",
      "Epoch 285/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4059 - acc: 0.7964 - val_loss: 0.4176 - val_acc: 0.8117\n",
      "Epoch 286/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4072 - acc: 0.7948 - val_loss: 0.4166 - val_acc: 0.8117\n",
      "Epoch 287/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4064 - acc: 0.7980 - val_loss: 0.4201 - val_acc: 0.8247\n",
      "Epoch 288/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.4063 - acc: 0.7964 - val_loss: 0.4177 - val_acc: 0.8117\n",
      "Epoch 289/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4055 - acc: 0.8062 - val_loss: 0.4170 - val_acc: 0.8117\n",
      "Epoch 290/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4052 - acc: 0.7932 - val_loss: 0.4166 - val_acc: 0.8182\n",
      "Epoch 291/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4052 - acc: 0.7980 - val_loss: 0.4152 - val_acc: 0.8247\n",
      "Epoch 292/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4059 - acc: 0.7964 - val_loss: 0.4170 - val_acc: 0.8182\n",
      "Epoch 293/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4058 - acc: 0.8013 - val_loss: 0.4168 - val_acc: 0.8117\n",
      "Epoch 294/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4035 - acc: 0.7980 - val_loss: 0.4154 - val_acc: 0.8182\n",
      "Epoch 295/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4040 - acc: 0.7964 - val_loss: 0.4149 - val_acc: 0.8247\n",
      "Epoch 296/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4047 - acc: 0.7964 - val_loss: 0.4149 - val_acc: 0.8247\n",
      "Epoch 297/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.4047 - acc: 0.7997 - val_loss: 0.4165 - val_acc: 0.8182\n",
      "Epoch 298/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 44us/step - loss: 0.4037 - acc: 0.8029 - val_loss: 0.4169 - val_acc: 0.8182\n",
      "Epoch 299/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4039 - acc: 0.7932 - val_loss: 0.4162 - val_acc: 0.8182\n",
      "Epoch 300/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4041 - acc: 0.7964 - val_loss: 0.4153 - val_acc: 0.8247\n",
      "Epoch 301/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.4045 - acc: 0.7932 - val_loss: 0.4155 - val_acc: 0.8182\n",
      "Epoch 302/400\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.4022 - acc: 0.8062 - val_loss: 0.4184 - val_acc: 0.8312\n",
      "Epoch 303/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.4034 - acc: 0.8013 - val_loss: 0.4206 - val_acc: 0.8312\n",
      "Epoch 304/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4023 - acc: 0.8046 - val_loss: 0.4194 - val_acc: 0.8312\n",
      "Epoch 305/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4014 - acc: 0.7980 - val_loss: 0.4206 - val_acc: 0.8312\n",
      "Epoch 306/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4043 - acc: 0.7980 - val_loss: 0.4158 - val_acc: 0.8247\n",
      "Epoch 307/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4018 - acc: 0.8029 - val_loss: 0.4138 - val_acc: 0.8247\n",
      "Epoch 308/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4023 - acc: 0.7997 - val_loss: 0.4151 - val_acc: 0.8247\n",
      "Epoch 309/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.4025 - acc: 0.8046 - val_loss: 0.4153 - val_acc: 0.8247\n",
      "Epoch 310/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.4021 - acc: 0.7932 - val_loss: 0.4160 - val_acc: 0.8182\n",
      "Epoch 311/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.4017 - acc: 0.8013 - val_loss: 0.4153 - val_acc: 0.8182\n",
      "Epoch 312/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4014 - acc: 0.7997 - val_loss: 0.4140 - val_acc: 0.8182\n",
      "Epoch 313/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.4019 - acc: 0.7964 - val_loss: 0.4153 - val_acc: 0.8117\n",
      "Epoch 314/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.3984 - acc: 0.8062 - val_loss: 0.4141 - val_acc: 0.8247\n",
      "Epoch 315/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.4019 - acc: 0.8029 - val_loss: 0.4124 - val_acc: 0.8182\n",
      "Epoch 316/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3994 - acc: 0.8029 - val_loss: 0.4132 - val_acc: 0.8182\n",
      "Epoch 317/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4003 - acc: 0.8029 - val_loss: 0.4143 - val_acc: 0.8247\n",
      "Epoch 318/400\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.4006 - acc: 0.7980 - val_loss: 0.4148 - val_acc: 0.8182\n",
      "Epoch 319/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.4001 - acc: 0.8029 - val_loss: 0.4133 - val_acc: 0.8247\n",
      "Epoch 320/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.3987 - acc: 0.8029 - val_loss: 0.4124 - val_acc: 0.8182\n",
      "Epoch 321/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3993 - acc: 0.8062 - val_loss: 0.4129 - val_acc: 0.8182\n",
      "Epoch 322/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.3989 - acc: 0.8046 - val_loss: 0.4121 - val_acc: 0.8182\n",
      "Epoch 323/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.4008 - acc: 0.8029 - val_loss: 0.4117 - val_acc: 0.8182\n",
      "Epoch 324/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3995 - acc: 0.7980 - val_loss: 0.4096 - val_acc: 0.8247\n",
      "Epoch 325/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.3977 - acc: 0.7948 - val_loss: 0.4141 - val_acc: 0.8117\n",
      "Epoch 326/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3994 - acc: 0.8062 - val_loss: 0.4087 - val_acc: 0.8247\n",
      "Epoch 327/400\n",
      "614/614 [==============================] - 0s 37us/step - loss: 0.4003 - acc: 0.8046 - val_loss: 0.4097 - val_acc: 0.8182\n",
      "Epoch 328/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3988 - acc: 0.7980 - val_loss: 0.4100 - val_acc: 0.8247\n",
      "Epoch 329/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.3979 - acc: 0.8029 - val_loss: 0.4125 - val_acc: 0.8182\n",
      "Epoch 330/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.3987 - acc: 0.8029 - val_loss: 0.4096 - val_acc: 0.8182\n",
      "Epoch 331/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.3998 - acc: 0.8013 - val_loss: 0.4095 - val_acc: 0.8247\n",
      "Epoch 332/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.3977 - acc: 0.8013 - val_loss: 0.4101 - val_acc: 0.8312\n",
      "Epoch 333/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3960 - acc: 0.8062 - val_loss: 0.4114 - val_acc: 0.8247\n",
      "Epoch 334/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.3969 - acc: 0.7964 - val_loss: 0.4085 - val_acc: 0.8182\n",
      "Epoch 335/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3962 - acc: 0.8062 - val_loss: 0.4105 - val_acc: 0.8247\n",
      "Epoch 336/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.3969 - acc: 0.7980 - val_loss: 0.4097 - val_acc: 0.8312\n",
      "Epoch 337/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3978 - acc: 0.8029 - val_loss: 0.4118 - val_acc: 0.8247\n",
      "Epoch 338/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.3956 - acc: 0.8046 - val_loss: 0.4150 - val_acc: 0.8377\n",
      "Epoch 339/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.3959 - acc: 0.8062 - val_loss: 0.4125 - val_acc: 0.8182\n",
      "Epoch 340/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.3978 - acc: 0.8013 - val_loss: 0.4119 - val_acc: 0.8182\n",
      "Epoch 341/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.3969 - acc: 0.8078 - val_loss: 0.4140 - val_acc: 0.8182\n",
      "Epoch 342/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.3951 - acc: 0.8046 - val_loss: 0.4122 - val_acc: 0.8182\n",
      "Epoch 343/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.3945 - acc: 0.8046 - val_loss: 0.4119 - val_acc: 0.8312\n",
      "Epoch 344/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.3962 - acc: 0.8029 - val_loss: 0.4086 - val_acc: 0.8182\n",
      "Epoch 345/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.3962 - acc: 0.8029 - val_loss: 0.4095 - val_acc: 0.8312\n",
      "Epoch 346/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.3955 - acc: 0.7997 - val_loss: 0.4091 - val_acc: 0.8182\n",
      "Epoch 347/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.3954 - acc: 0.8062 - val_loss: 0.4124 - val_acc: 0.8377\n",
      "Epoch 348/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.3966 - acc: 0.8013 - val_loss: 0.4076 - val_acc: 0.8247\n",
      "Epoch 349/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.3948 - acc: 0.8046 - val_loss: 0.4105 - val_acc: 0.8442\n",
      "Epoch 350/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3939 - acc: 0.8127 - val_loss: 0.4109 - val_acc: 0.8377\n",
      "Epoch 351/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.3961 - acc: 0.8013 - val_loss: 0.4090 - val_acc: 0.8312\n",
      "Epoch 352/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3934 - acc: 0.8046 - val_loss: 0.4064 - val_acc: 0.8247\n",
      "Epoch 353/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.3954 - acc: 0.8094 - val_loss: 0.4079 - val_acc: 0.8247\n",
      "Epoch 354/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3935 - acc: 0.8111 - val_loss: 0.4076 - val_acc: 0.8247\n",
      "Epoch 355/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3939 - acc: 0.8062 - val_loss: 0.4105 - val_acc: 0.8377\n",
      "Epoch 356/400\n",
      "614/614 [==============================] - 0s 36us/step - loss: 0.3937 - acc: 0.7997 - val_loss: 0.4066 - val_acc: 0.8247\n",
      "Epoch 357/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.3933 - acc: 0.8046 - val_loss: 0.4055 - val_acc: 0.8247\n",
      "Epoch 358/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 42us/step - loss: 0.3937 - acc: 0.8078 - val_loss: 0.4071 - val_acc: 0.8247\n",
      "Epoch 359/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3933 - acc: 0.8111 - val_loss: 0.4058 - val_acc: 0.8247\n",
      "Epoch 360/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.3930 - acc: 0.8062 - val_loss: 0.4085 - val_acc: 0.8442\n",
      "Epoch 361/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.3929 - acc: 0.8029 - val_loss: 0.4066 - val_acc: 0.8247\n",
      "Epoch 362/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.3941 - acc: 0.8062 - val_loss: 0.4074 - val_acc: 0.8312\n",
      "Epoch 363/400\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.3913 - acc: 0.7997 - val_loss: 0.4131 - val_acc: 0.8117\n",
      "Epoch 364/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.3939 - acc: 0.8046 - val_loss: 0.4100 - val_acc: 0.8182\n",
      "Epoch 365/400\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.3904 - acc: 0.8029 - val_loss: 0.4185 - val_acc: 0.8312\n",
      "Epoch 366/400\n",
      "614/614 [==============================] - 0s 62us/step - loss: 0.3932 - acc: 0.8029 - val_loss: 0.4059 - val_acc: 0.8247\n",
      "Epoch 367/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.3917 - acc: 0.8046 - val_loss: 0.4045 - val_acc: 0.8247\n",
      "Epoch 368/400\n",
      "614/614 [==============================] - 0s 70us/step - loss: 0.3926 - acc: 0.8046 - val_loss: 0.4063 - val_acc: 0.8247\n",
      "Epoch 369/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.3910 - acc: 0.8046 - val_loss: 0.4078 - val_acc: 0.8442\n",
      "Epoch 370/400\n",
      "614/614 [==============================] - 0s 65us/step - loss: 0.3921 - acc: 0.8029 - val_loss: 0.4060 - val_acc: 0.8247\n",
      "Epoch 371/400\n",
      "614/614 [==============================] - 0s 63us/step - loss: 0.3899 - acc: 0.8078 - val_loss: 0.4061 - val_acc: 0.8247\n",
      "Epoch 372/400\n",
      "614/614 [==============================] - 0s 73us/step - loss: 0.3897 - acc: 0.8062 - val_loss: 0.4054 - val_acc: 0.8312\n",
      "Epoch 373/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.3900 - acc: 0.8029 - val_loss: 0.4123 - val_acc: 0.8182\n",
      "Epoch 374/400\n",
      "614/614 [==============================] - 0s 49us/step - loss: 0.3916 - acc: 0.8029 - val_loss: 0.4080 - val_acc: 0.8182\n",
      "Epoch 375/400\n",
      "614/614 [==============================] - 0s 55us/step - loss: 0.3904 - acc: 0.8127 - val_loss: 0.4072 - val_acc: 0.8377\n",
      "Epoch 376/400\n",
      "614/614 [==============================] - 0s 75us/step - loss: 0.3897 - acc: 0.8046 - val_loss: 0.4047 - val_acc: 0.8312\n",
      "Epoch 377/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.3908 - acc: 0.8029 - val_loss: 0.4056 - val_acc: 0.8247\n",
      "Epoch 378/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3909 - acc: 0.8046 - val_loss: 0.4053 - val_acc: 0.8247\n",
      "Epoch 379/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.3892 - acc: 0.8029 - val_loss: 0.4107 - val_acc: 0.8377\n",
      "Epoch 380/400\n",
      "614/614 [==============================] - 0s 57us/step - loss: 0.3898 - acc: 0.8111 - val_loss: 0.4066 - val_acc: 0.8247\n",
      "Epoch 381/400\n",
      "614/614 [==============================] - 0s 58us/step - loss: 0.3891 - acc: 0.8013 - val_loss: 0.4117 - val_acc: 0.8117\n",
      "Epoch 382/400\n",
      "614/614 [==============================] - 0s 60us/step - loss: 0.3896 - acc: 0.8013 - val_loss: 0.4060 - val_acc: 0.8247\n",
      "Epoch 383/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3885 - acc: 0.7997 - val_loss: 0.4083 - val_acc: 0.8182\n",
      "Epoch 384/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.3902 - acc: 0.8013 - val_loss: 0.4066 - val_acc: 0.8247\n",
      "Epoch 385/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.3883 - acc: 0.8046 - val_loss: 0.4077 - val_acc: 0.8247\n",
      "Epoch 386/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.3872 - acc: 0.8013 - val_loss: 0.4076 - val_acc: 0.8247\n",
      "Epoch 387/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3877 - acc: 0.8013 - val_loss: 0.4105 - val_acc: 0.8117\n",
      "Epoch 388/400\n",
      "614/614 [==============================] - 0s 54us/step - loss: 0.3888 - acc: 0.8062 - val_loss: 0.4069 - val_acc: 0.8182\n",
      "Epoch 389/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3890 - acc: 0.8127 - val_loss: 0.4051 - val_acc: 0.8377\n",
      "Epoch 390/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.3903 - acc: 0.8046 - val_loss: 0.4009 - val_acc: 0.8247\n",
      "Epoch 391/400\n",
      "614/614 [==============================] - 0s 39us/step - loss: 0.3874 - acc: 0.8062 - val_loss: 0.4019 - val_acc: 0.8247\n",
      "Epoch 392/400\n",
      "614/614 [==============================] - 0s 47us/step - loss: 0.3894 - acc: 0.8062 - val_loss: 0.4030 - val_acc: 0.8247\n",
      "Epoch 393/400\n",
      "614/614 [==============================] - 0s 41us/step - loss: 0.3884 - acc: 0.8013 - val_loss: 0.4061 - val_acc: 0.8182\n",
      "Epoch 394/400\n",
      "614/614 [==============================] - 0s 50us/step - loss: 0.3892 - acc: 0.8046 - val_loss: 0.4067 - val_acc: 0.8312\n",
      "Epoch 395/400\n",
      "614/614 [==============================] - 0s 52us/step - loss: 0.3871 - acc: 0.8094 - val_loss: 0.4061 - val_acc: 0.8247\n",
      "Epoch 396/400\n",
      "614/614 [==============================] - 0s 45us/step - loss: 0.3878 - acc: 0.7997 - val_loss: 0.4063 - val_acc: 0.8182\n",
      "Epoch 397/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3875 - acc: 0.8111 - val_loss: 0.4052 - val_acc: 0.8312\n",
      "Epoch 398/400\n",
      "614/614 [==============================] - 0s 42us/step - loss: 0.3868 - acc: 0.7997 - val_loss: 0.4102 - val_acc: 0.7987\n",
      "Epoch 399/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3873 - acc: 0.8111 - val_loss: 0.4046 - val_acc: 0.8312\n",
      "Epoch 400/400\n",
      "614/614 [==============================] - 0s 44us/step - loss: 0.3866 - acc: 0.8046 - val_loss: 0.4078 - val_acc: 0.8377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b6580fba20>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knn=pd.DataFrame(data=fancyimpute.KNN(k=8).fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation='relu', input_dim=8))\n",
    "\n",
    "model.add(Dense(10,activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(df_knn[input_columns], df[8], batch_size=32, epochs=400, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from this highly unscientific test, the common wisdom that mean imputation is just as good is not necessarily true. Even with this overkill of a model, KNN imputed data performs significantly better than mean imputed data(0.8701 - epoch 396 vs 0.7987 - epoch 324 in this run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data is broadly classified into three categories: MCAR, MAR and MNAR. We show the abysmal performance of mean imputation and median imputation with a toy example. Next, we create an intuitive understanding of KNN imputation and write sample code for its implementation. \n",
    "\n",
    "Finally, we apply the techniques to Pima Indian Diabetes set and use four different imputation strategies. We show the superiority of KNN imputation technique over other imputation strategies for both logistic regression and neural networks, discrediting a common belief about imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Further Reading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - https://pypi.org/project/fancyimpute/\n",
    " - https://github.com/iskandr/fancyimpute/tree/master/fancyimpute\n",
    " - https://github.com/iskandr/knnimpute/blob/master/knnimpute/few_observed_entries.py\n",
    " - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959387/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
